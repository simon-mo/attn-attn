{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from AttentionModule import Conv2d_Attn\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_pretrained = models.resnet50(pretrained=True)\n",
    "nn.Conv2d = Conv2d_Attn\n",
    "resnet_attn = models.resnet50()\n",
    "resnet_attn.load_state_dict(resnet_pretrained.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block turns 'layer1.0.downsample.0.weight' to 'layer1[0].downsample[0].weight'\n",
    "param_keys = list(resnet_attn.state_dict().keys())\n",
    "formatted_keys = []\n",
    "for k in param_keys:\n",
    "    found = re.findall(r'\\.[\\d]{1,2}\\.', k)\n",
    "    if len(found):\n",
    "        for f in found:\n",
    "            k = k.replace(f, '[{}].'.format(f.strip('.')))\n",
    "    formatted_keys.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block turn off gradient up for all params except attn_weights\n",
    "def turn_off_grad_except(lst=[]):\n",
    "    for k in formatted_keys:\n",
    "        obj = eval('resnet_attn.'+k)\n",
    "        for kw in lst:\n",
    "            if not kw in k:\n",
    "                obj.requires_grad = False\n",
    "            else:\n",
    "                obj.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn.fc = nn.Linear(resnet_attn.fc.in_features, 144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     normalize])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root='../data/train', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "valset = torchvision.datasets.ImageFolder(root='../data/val', transform=transform)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_imgs = len(trainset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn = resnet_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of attention parameters 26560\n"
     ]
    }
   ],
   "source": [
    "total_attn_params = 0\n",
    "for k in formatted_keys:\n",
    "    obj = eval('resnet_attn.'+k)\n",
    "    if 'attn_weights' in k:\n",
    "        total_attn_params += np.prod(obj.shape)\n",
    "print(\"Total number of attention parameters\", total_attn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the attention parameters to diverge from 1, therefore we penalize element-wise square loss as $\\lambda (1 \\times \\text{# params} - (x - 1)^2)$\n",
    "\n",
    "But this is too big a number,\n",
    "let's try: \n",
    "$- (x - 1)^2$ for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lambda = 1e-2 #set default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_objs(name, net='resnet_attn'):\n",
    "    res = []\n",
    "    for k in formatted_keys:\n",
    "        obj = eval(f'{net}.'+k)\n",
    "        if name in k:\n",
    "            res.append(obj)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attn_loss(n_params=26560):\n",
    "    attns = get_params_objs('attn_weights')\n",
    "    penality = sum([torch.pow(t - 1,2).mean() for t in attns])\n",
    "    return _lambda*(- penality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_top3(train=True, val=True):\n",
    "    if train:\n",
    "        correct_count = 0\n",
    "        for inp, label in tqdm(iter(trainloader)):\n",
    "            _, idx = resnet_attn(Variable(inp).cuda()).topk(3)\n",
    "            lab = Variable(label).cuda()\n",
    "            lab_expand = lab.unsqueeze(1).expand_as(idx)\n",
    "            correct_count += int((idx == lab_expand).sum())\n",
    "        print(correct_count/len(trainset))\n",
    "    \n",
    "    if val:\n",
    "        correct_count = 0\n",
    "        for inp, label in tqdm(iter(valloader)):\n",
    "            _, idx = resnet_attn(Variable(inp).cuda()).topk(3)\n",
    "            lab = Variable(label).cuda()\n",
    "            lab_expand = lab.unsqueeze(1).expand_as(idx)\n",
    "            correct_count += int((idx == lab_expand).sum())\n",
    "        print(correct_count/len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attn_hist():\n",
    "    attns = get_params_objs('attn_weights')\n",
    "    attns = torch.cat([attn.squeeze() for attn in attns])\n",
    "    attns_arr = attns.data.cpu().numpy()\n",
    "    plt.hist(attns_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(k=1, add_attn=True, score=True, plot_hist=False):\n",
    "    cls_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet_attn.parameters()))\n",
    "    \n",
    "    for j in range(k):\n",
    "        running_loss = 0.0\n",
    "        running_attn_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = resnet_attn(inputs)\n",
    "            loss = cls_criterion(outputs, labels)\n",
    "            attn_loss = compute_attn_loss()\n",
    "            if add_attn:\n",
    "                loss += attn_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.data[0]\n",
    "            running_attn_loss += attn_loss.data[0]\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print('[%5d] iter, [%2f] epoch, avg loss: %.3f, attn_loss: %.5f ' %\n",
    "                      (i + 1, i*batch_size/total_imgs, running_loss/print_every, running_attn_loss/print_every))\n",
    "                running_loss = 0.0\n",
    "                running_attn_loss = 0.0\n",
    "        if score:\n",
    "            score_top3()\n",
    "        if plot_hist:\n",
    "            plot_attn_hist()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, avg loss: 0.101, attn_loss: 0.00000 \n",
      "[   51] iter, [0.196054] epoch, avg loss: 4.239, attn_loss: 0.00000 \n",
      "[  101] iter, [0.392109] epoch, avg loss: 3.486, attn_loss: 0.00000 \n",
      "[  151] iter, [0.588163] epoch, avg loss: 3.236, attn_loss: 0.00000 \n",
      "[  201] iter, [0.784218] epoch, avg loss: 3.051, attn_loss: 0.00000 \n",
      "[  251] iter, [0.980272] epoch, avg loss: 2.987, attn_loss: 0.00000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [01:04<00:00,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5934321774292366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 55/55 [00:14<00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5119726339794755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "turn_off_grad_except(['fc'])\n",
    "resnet_attn.eval() # Turn on batchnorm\n",
    "train(1, add_attn=False, plot_hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, avg loss: 0.047, attn_loss: 0.00000 \n",
      "[   51] iter, [0.196054] epoch, avg loss: 2.285, attn_loss: -0.00294 \n",
      "[  101] iter, [0.392109] epoch, avg loss: 2.299, attn_loss: -0.00902 \n",
      "[  151] iter, [0.588163] epoch, avg loss: 2.287, attn_loss: -0.01449 \n",
      "[  201] iter, [0.784218] epoch, avg loss: 2.166, attn_loss: -0.02115 \n",
      "[  251] iter, [0.980272] epoch, avg loss: 2.221, attn_loss: -0.02794 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [01:04<00:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6630314912388188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 55/55 [00:14<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5775370581527937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, avg loss: 0.044, attn_loss: -0.00064 \n",
      "[   51] iter, [0.196054] epoch, avg loss: 2.046, attn_loss: -0.03668 \n",
      "[  101] iter, [0.392109] epoch, avg loss: 2.044, attn_loss: -0.04625 \n",
      "[  151] iter, [0.588163] epoch, avg loss: 2.160, attn_loss: -0.05567 \n",
      "[  201] iter, [0.784218] epoch, avg loss: 2.159, attn_loss: -0.06456 \n",
      "[  251] iter, [0.980272] epoch, avg loss: 2.043, attn_loss: -0.07294 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [01:04<00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6848425438059061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 55/55 [00:14<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5860889395667047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, avg loss: 0.028, attn_loss: -0.00158 \n",
      "[   51] iter, [0.196054] epoch, avg loss: 2.011, attn_loss: -0.08489 \n",
      "[  101] iter, [0.392109] epoch, avg loss: 2.045, attn_loss: -0.09638 \n",
      "[  151] iter, [0.588163] epoch, avg loss: 1.983, attn_loss: -0.10647 \n",
      "[  201] iter, [0.784218] epoch, avg loss: 2.004, attn_loss: -0.11811 \n",
      "[  251] iter, [0.980272] epoch, avg loss: 2.014, attn_loss: -0.12928 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 115/256 [00:54<01:06,  2.12it/s]Process Process-22:\n",
      "Traceback (most recent call last):\n",
      "Process Process-21:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-21-18600e2e4153>\", line 4, in <module>\n",
      "    train(10, add_attn=False, plot_hist=False)\n",
      "  File \"<ipython-input-19-0f6e34592bc7>\", line 31, in train\n",
      "    score_top3()\n",
      "  File \"<ipython-input-17-41c6e7e1f1d4>\", line 6, in score_top3\n",
      "    lab = Variable(label).cuda()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py\", line 298, in cuda\n",
      "    return CudaTransfer.apply(self, device, async)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\", line 201, in forward\n",
      "    return i.cuda(async=async)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/torch/_utils.py\", line 69, in _cuda\n",
      "    return new_type(self.size()).copy_(self, async)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/inspect.py\", line 1445, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 224, in findsource\n",
      "    if pmatch(lines[lnum]):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 23693) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "turn_off_grad_except(['attn_weights'])\n",
    "resnet_attn.eval() # Turn on batchnorm\n",
    "_lambda=1\n",
    "train(10, add_attn=False, plot_hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet_attn, '58_6_val_attn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d_Attn(\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d_Attn(\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d_Attn(\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d_Attn(\n",
       "        )\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d_Attn(\n",
       "        )\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d_Attn(\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d_Attn(\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d_Attn(\n",
       "      )\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  (fc): Linear(in_features=2048, out_features=144, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
