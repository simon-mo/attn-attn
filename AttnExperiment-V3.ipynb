{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from AttentionModule import Conv2d_Attn\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_pretrained = models.resnet50(pretrained=True)\n",
    "nn.Conv2d = Conv2d_Attn\n",
    "resnet_attn = models.resnet50()\n",
    "resnet_attn.load_state_dict(resnet_pretrained.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block turns 'layer1.0.downsample.0.weight' to 'layer1[0].downsample[0].weight'\n",
    "param_keys = list(resnet_attn.state_dict().keys())\n",
    "formatted_keys = []\n",
    "for k in param_keys:\n",
    "    found = re.findall(r'\\.[\\d]{1,2}\\.', k)\n",
    "    if len(found):\n",
    "        for f in found:\n",
    "            k = k.replace(f, '[{}].'.format(f.strip('.')))\n",
    "    formatted_keys.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block turn off gradient up for all params except attn_weights\n",
    "def turn_off_grad_except(lst=[]):\n",
    "    turning_off_keys = [k for k in formatted_keys for l in lst if l in k]\n",
    "    for k in formatted_keys:\n",
    "        obj = eval('resnet_attn.'+k)\n",
    "        if k in turning_off_keys:\n",
    "            obj.requires_grad = True\n",
    "        else:\n",
    "            obj.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn.fc = nn.Linear(resnet_attn.fc.in_features, 144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     normalize])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root='./data/train', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_imgs = len(trainset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn = resnet_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of attention parameters 26560\n"
     ]
    }
   ],
   "source": [
    "total_attn_params = 0\n",
    "for k in formatted_keys:\n",
    "    obj = eval('resnet_attn.'+k)\n",
    "    if 'attn_weights' in k:\n",
    "        total_attn_params += np.prod(obj.shape)\n",
    "print(\"Total number of attention parameters\", total_attn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the attention parameters to diverge from 1, therefore we penalize element-wise square loss as $\\lambda (1 \\times \\text{# params} - (x - 1)^2)$\n",
    "\n",
    "But this is too big a number,\n",
    "let's try: \n",
    "$- (x - 1)^2$ for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lambda = 1e-1 #set default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_objs(name, net='resnet_attn'):\n",
    "    res = []\n",
    "    for k in formatted_keys:\n",
    "        obj = eval(f'{net}.'+k)\n",
    "        if name in k:\n",
    "            res.append(obj)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attn_loss(n_params=26560):\n",
    "    attns = get_params_objs('attn_weights')\n",
    "#     penality = sum([torch.pow(t - 1,2).mean() for t in attns])\n",
    "    penality = (1000/n_params)*sum([torch.min(torch.pow(t-2, 2), torch.pow(t, 2)).sum() for t in attns])\n",
    "    return (_lambda)*(penality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_params_objs('attn_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 100.0000\n",
       "[torch.cuda.FloatTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_attn_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_k_epoch(k, add_attn=True, score_epoch=False):\n",
    "    cls_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet_attn.parameters()))\n",
    "    \n",
    "    for epoch in range(k):\n",
    "        running_loss = 0.0\n",
    "        running_attn_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = resnet_attn(inputs)\n",
    "            clf_loss = cls_criterion(outputs, labels)\n",
    "                        \n",
    "            attn_loss = compute_attn_loss()\n",
    "            \n",
    "            if add_attn:\n",
    "                loss = clf_loss + attn_loss\n",
    "            else:\n",
    "                loss = clf_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += clf_loss.data[0]\n",
    "            running_attn_loss += attn_loss.data[0]\n",
    "\n",
    "            if i % print_every == 0:\n",
    "                print('[%5d] iter, [%2f] epoch, classifer loss: %.3f, attn_loss: %.5f ' %\n",
    "                      (i + 1, i*batch_size/total_imgs, running_loss/print_every, running_attn_loss/print_every))\n",
    "                running_loss = 0.0\n",
    "                running_attn_loss = 0.0\n",
    "        if score_epoch:\n",
    "            print(score(batch_size=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_one_epoch(add_attn=True):\n",
    "#     cls_criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet_attn.parameters()))\n",
    "    \n",
    "#     running_loss = 0.0\n",
    "#     running_attn_loss = 0.0\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         inputs, labels = data\n",
    "#         inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = resnet_attn(inputs)\n",
    "#         clf_loss = cls_criterion(outputs, labels)\n",
    "#         attn_loss = compute_attn_loss()\n",
    "#         if add_attn:\n",
    "#             loss = clf_loss + attn_loss\n",
    "#         else:\n",
    "#             loss = clf_loss\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += clf_loss.data[0]\n",
    "#         running_attn_loss += attn_loss.data[0]\n",
    "\n",
    "#         if i % print_every == 0:\n",
    "#             print('[%5d] iter, [%2f] epoch, classifer loss: %.3f, attn_loss: %.5f ' %\n",
    "#                   (i + 1, i*batch_size/total_imgs, running_loss/print_every, running_attn_loss/print_every))\n",
    "#             running_loss = 0.0\n",
    "#             running_attn_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def score(net=resnet_attn, batch_size=batch_size):\n",
    "    trainset = torchvision.datasets.ImageFolder(root='./data/train', transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    \n",
    "    valset = torchvision.datasets.ImageFolder(root='./data/val', transform=transform)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    \n",
    "    train_correct = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    for inp, label in tqdm(iter(trainloader)):\n",
    "        _, idx = net(Variable(inp).cuda()).max(1)\n",
    "        train_correct += int(sum(idx.cpu().data == label))\n",
    "    \n",
    "    for inp, label in tqdm(iter(valloader)):\n",
    "        _, idx = net(Variable(inp).cuda()).max(1)\n",
    "        val_correct += int(sum(idx.cpu().data == label))\n",
    "    \n",
    "    return {\n",
    "        'train_accu': train_correct/len(trainset),\n",
    "        'val_accu': val_correct/len(valset)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a fresh fc layer. \n",
    "`turn_off_grad_except([])` turns off grads for all weights but the fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, classifer loss: 0.996, attn_loss: 20.00000 \n",
      "[    6] iter, [0.019605] epoch, classifer loss: 4.831, attn_loss: 99.40570 \n",
      "[   11] iter, [0.039211] epoch, classifer loss: 4.762, attn_loss: 98.43576 \n",
      "[   16] iter, [0.058816] epoch, classifer loss: 4.679, attn_loss: 97.47825 \n",
      "[   21] iter, [0.078422] epoch, classifer loss: 4.451, attn_loss: 96.52525 \n",
      "[   26] iter, [0.098027] epoch, classifer loss: 4.258, attn_loss: 95.57863 \n",
      "[   31] iter, [0.117633] epoch, classifer loss: 4.257, attn_loss: 94.63789 \n",
      "[   36] iter, [0.137238] epoch, classifer loss: 3.738, attn_loss: 93.70402 \n",
      "[   41] iter, [0.156844] epoch, classifer loss: 4.058, attn_loss: 92.77974 \n",
      "[   46] iter, [0.176449] epoch, classifer loss: 3.857, attn_loss: 91.86414 \n",
      "[   51] iter, [0.196054] epoch, classifer loss: 3.884, attn_loss: 90.95941 \n",
      "[   56] iter, [0.215660] epoch, classifer loss: 3.618, attn_loss: 90.06505 \n",
      "[   61] iter, [0.235265] epoch, classifer loss: 3.912, attn_loss: 89.17942 \n",
      "[   66] iter, [0.254871] epoch, classifer loss: 3.805, attn_loss: 88.30068 \n",
      "[   71] iter, [0.274476] epoch, classifer loss: 3.606, attn_loss: 87.42948 \n",
      "[   76] iter, [0.294082] epoch, classifer loss: 3.557, attn_loss: 86.56927 \n",
      "[   81] iter, [0.313687] epoch, classifer loss: 3.582, attn_loss: 85.71860 \n",
      "[   86] iter, [0.333292] epoch, classifer loss: 3.502, attn_loss: 84.87376 \n",
      "[   91] iter, [0.352898] epoch, classifer loss: 3.139, attn_loss: 84.03923 \n",
      "[   96] iter, [0.372503] epoch, classifer loss: 3.476, attn_loss: 83.21151 \n",
      "[  101] iter, [0.392109] epoch, classifer loss: 3.454, attn_loss: 82.38949 \n",
      "[  106] iter, [0.411714] epoch, classifer loss: 3.444, attn_loss: 81.57380 \n",
      "[  111] iter, [0.431320] epoch, classifer loss: 3.431, attn_loss: 80.76352 \n",
      "[  116] iter, [0.450925] epoch, classifer loss: 3.530, attn_loss: 79.95768 \n",
      "[  121] iter, [0.470531] epoch, classifer loss: 3.371, attn_loss: 79.15829 \n",
      "[  126] iter, [0.490136] epoch, classifer loss: 3.468, attn_loss: 78.36886 \n",
      "[  131] iter, [0.509741] epoch, classifer loss: 3.344, attn_loss: 77.58845 \n",
      "[  136] iter, [0.529347] epoch, classifer loss: 3.274, attn_loss: 76.81467 \n",
      "[  141] iter, [0.548952] epoch, classifer loss: 3.210, attn_loss: 76.05062 \n",
      "[  146] iter, [0.568558] epoch, classifer loss: 3.391, attn_loss: 75.29518 \n",
      "[  151] iter, [0.588163] epoch, classifer loss: 3.498, attn_loss: 74.54212 \n",
      "[  156] iter, [0.607769] epoch, classifer loss: 3.045, attn_loss: 73.79378 \n",
      "[  161] iter, [0.627374] epoch, classifer loss: 3.348, attn_loss: 73.05743 \n",
      "[  166] iter, [0.646980] epoch, classifer loss: 3.624, attn_loss: 72.32580 \n",
      "[  171] iter, [0.666585] epoch, classifer loss: 3.475, attn_loss: 71.59556 \n",
      "[  176] iter, [0.686190] epoch, classifer loss: 3.292, attn_loss: 70.87048 \n",
      "[  181] iter, [0.705796] epoch, classifer loss: 3.113, attn_loss: 70.15603 \n",
      "[  186] iter, [0.725401] epoch, classifer loss: 3.310, attn_loss: 69.45483 \n",
      "[  191] iter, [0.745007] epoch, classifer loss: 3.139, attn_loss: 68.75912 \n",
      "[  196] iter, [0.764612] epoch, classifer loss: 3.505, attn_loss: 68.06690 \n",
      "[  201] iter, [0.784218] epoch, classifer loss: 3.345, attn_loss: 67.37437 \n",
      "[  206] iter, [0.803823] epoch, classifer loss: 3.544, attn_loss: 66.68436 \n",
      "[  211] iter, [0.823429] epoch, classifer loss: 3.190, attn_loss: 66.00296 \n",
      "[  216] iter, [0.843034] epoch, classifer loss: 3.424, attn_loss: 65.33303 \n",
      "[  221] iter, [0.862639] epoch, classifer loss: 3.424, attn_loss: 64.67080 \n",
      "[  226] iter, [0.882245] epoch, classifer loss: 3.459, attn_loss: 64.01391 \n",
      "[  231] iter, [0.901850] epoch, classifer loss: 3.341, attn_loss: 63.36207 \n",
      "[  236] iter, [0.921456] epoch, classifer loss: 3.280, attn_loss: 62.71610 \n",
      "[  241] iter, [0.941061] epoch, classifer loss: 3.389, attn_loss: 62.07745 \n",
      "[  246] iter, [0.960667] epoch, classifer loss: 2.944, attn_loss: 61.44007 \n",
      "[  251] iter, [0.980272] epoch, classifer loss: 3.518, attn_loss: 60.80800 \n",
      "[  256] iter, [0.999877] epoch, classifer loss: 3.635, attn_loss: 60.17779 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [01:03<00:00,  4.04it/s]\n",
      "100%|██████████| 55/55 [00:13<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_accu': 0.24200465629212106, 'val_accu': 0.2394526795895097}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, classifer loss: 0.664, attn_loss: 11.96036 \n",
      "[    6] iter, [0.019605] epoch, classifer loss: 3.123, attn_loss: 59.43118 \n",
      "[   11] iter, [0.039211] epoch, classifer loss: 3.262, attn_loss: 58.82069 \n",
      "[   16] iter, [0.058816] epoch, classifer loss: 3.433, attn_loss: 58.21531 \n",
      "[   21] iter, [0.078422] epoch, classifer loss: 3.313, attn_loss: 57.61526 \n",
      "[   26] iter, [0.098027] epoch, classifer loss: 3.285, attn_loss: 57.02174 \n",
      "[   31] iter, [0.117633] epoch, classifer loss: 3.444, attn_loss: 56.43397 \n",
      "[   36] iter, [0.137238] epoch, classifer loss: 3.386, attn_loss: 55.84815 \n",
      "[   41] iter, [0.156844] epoch, classifer loss: 3.427, attn_loss: 55.26619 \n",
      "[   46] iter, [0.176449] epoch, classifer loss: 3.463, attn_loss: 54.68930 \n",
      "[   51] iter, [0.196054] epoch, classifer loss: 3.310, attn_loss: 54.11746 \n",
      "[   56] iter, [0.215660] epoch, classifer loss: 3.388, attn_loss: 53.55119 \n",
      "[   61] iter, [0.235265] epoch, classifer loss: 3.397, attn_loss: 52.98815 \n",
      "[   66] iter, [0.254871] epoch, classifer loss: 3.243, attn_loss: 52.43038 \n",
      "[   71] iter, [0.274476] epoch, classifer loss: 3.429, attn_loss: 51.87930 \n",
      "[   76] iter, [0.294082] epoch, classifer loss: 3.373, attn_loss: 51.33361 \n",
      "[   81] iter, [0.313687] epoch, classifer loss: 3.262, attn_loss: 50.79306 \n",
      "[   86] iter, [0.333292] epoch, classifer loss: 3.565, attn_loss: 50.25499 \n",
      "[   91] iter, [0.352898] epoch, classifer loss: 3.311, attn_loss: 49.71984 \n",
      "[   96] iter, [0.372503] epoch, classifer loss: 3.303, attn_loss: 49.18749 \n",
      "[  101] iter, [0.392109] epoch, classifer loss: 3.434, attn_loss: 48.66140 \n",
      "[  106] iter, [0.411714] epoch, classifer loss: 3.525, attn_loss: 48.14175 \n",
      "[  111] iter, [0.431320] epoch, classifer loss: 3.479, attn_loss: 47.62272 \n",
      "[  116] iter, [0.450925] epoch, classifer loss: 3.575, attn_loss: 47.10510 \n",
      "[  121] iter, [0.470531] epoch, classifer loss: 3.421, attn_loss: 46.59235 \n",
      "[  126] iter, [0.490136] epoch, classifer loss: 3.257, attn_loss: 46.08879 \n",
      "[  131] iter, [0.509741] epoch, classifer loss: 3.510, attn_loss: 45.59171 \n",
      "[  136] iter, [0.529347] epoch, classifer loss: 3.275, attn_loss: 45.09920 \n",
      "[  141] iter, [0.548952] epoch, classifer loss: 3.295, attn_loss: 44.61191 \n",
      "[  146] iter, [0.568558] epoch, classifer loss: 3.773, attn_loss: 44.12646 \n",
      "[  151] iter, [0.588163] epoch, classifer loss: 3.571, attn_loss: 43.63832 \n",
      "[  156] iter, [0.607769] epoch, classifer loss: 3.776, attn_loss: 43.15103 \n",
      "[  161] iter, [0.627374] epoch, classifer loss: 3.484, attn_loss: 42.67169 \n",
      "[  166] iter, [0.646980] epoch, classifer loss: 3.517, attn_loss: 42.19823 \n",
      "[  171] iter, [0.666585] epoch, classifer loss: 3.867, attn_loss: 41.73096 \n",
      "[  176] iter, [0.686190] epoch, classifer loss: 3.561, attn_loss: 41.26766 \n",
      "[  181] iter, [0.705796] epoch, classifer loss: 3.467, attn_loss: 40.80832 \n",
      "[  186] iter, [0.725401] epoch, classifer loss: 3.645, attn_loss: 40.35246 \n",
      "[  191] iter, [0.745007] epoch, classifer loss: 3.589, attn_loss: 39.90173 \n",
      "[  196] iter, [0.764612] epoch, classifer loss: 3.786, attn_loss: 39.45285 \n",
      "[  201] iter, [0.784218] epoch, classifer loss: 3.663, attn_loss: 39.00583 \n",
      "[  206] iter, [0.803823] epoch, classifer loss: 3.666, attn_loss: 38.56430 \n",
      "[  211] iter, [0.823429] epoch, classifer loss: 3.700, attn_loss: 38.12793 \n",
      "[  216] iter, [0.843034] epoch, classifer loss: 3.736, attn_loss: 37.69390 \n",
      "[  221] iter, [0.862639] epoch, classifer loss: 3.684, attn_loss: 37.26396 \n",
      "[  226] iter, [0.882245] epoch, classifer loss: 3.757, attn_loss: 36.83812 \n",
      "[  231] iter, [0.901850] epoch, classifer loss: 3.676, attn_loss: 36.41520 \n",
      "[  236] iter, [0.921456] epoch, classifer loss: 3.814, attn_loss: 35.99629 \n",
      "[  241] iter, [0.941061] epoch, classifer loss: 3.788, attn_loss: 35.57980 \n",
      "[  246] iter, [0.960667] epoch, classifer loss: 3.736, attn_loss: 35.16586 \n",
      "[  251] iter, [0.980272] epoch, classifer loss: 3.842, attn_loss: 34.75640 \n",
      "[  256] iter, [0.999877] epoch, classifer loss: 3.769, attn_loss: 34.35029 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [01:03<00:00,  4.04it/s]\n",
      "100%|██████████| 55/55 [00:13<00:00,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_accu': 0.16591104031368706, 'val_accu': 0.1619156214367161}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, classifer loss: 0.731, attn_loss: 6.82167 \n",
      "[    6] iter, [0.019605] epoch, classifer loss: 3.771, attn_loss: 33.86945 \n",
      "[   11] iter, [0.039211] epoch, classifer loss: 3.663, attn_loss: 33.47488 \n",
      "[   16] iter, [0.058816] epoch, classifer loss: 3.886, attn_loss: 33.08380 \n",
      "[   21] iter, [0.078422] epoch, classifer loss: 3.682, attn_loss: 32.69592 \n",
      "[   26] iter, [0.098027] epoch, classifer loss: 3.740, attn_loss: 32.31085 \n",
      "[   31] iter, [0.117633] epoch, classifer loss: 3.768, attn_loss: 31.92910 \n",
      "[   36] iter, [0.137238] epoch, classifer loss: 3.587, attn_loss: 31.55175 \n",
      "[   41] iter, [0.156844] epoch, classifer loss: 3.697, attn_loss: 31.17883 \n",
      "[   46] iter, [0.176449] epoch, classifer loss: 3.866, attn_loss: 30.80954 \n",
      "[   51] iter, [0.196054] epoch, classifer loss: 3.682, attn_loss: 30.44321 \n",
      "[   56] iter, [0.215660] epoch, classifer loss: 3.725, attn_loss: 30.08019 \n",
      "[   61] iter, [0.235265] epoch, classifer loss: 3.801, attn_loss: 29.72103 \n",
      "[   66] iter, [0.254871] epoch, classifer loss: 3.806, attn_loss: 29.36575 \n",
      "[   71] iter, [0.274476] epoch, classifer loss: 4.136, attn_loss: 29.01316 \n",
      "[   76] iter, [0.294082] epoch, classifer loss: 3.882, attn_loss: 28.66332 \n",
      "[   81] iter, [0.313687] epoch, classifer loss: 3.891, attn_loss: 28.31753 \n",
      "[   86] iter, [0.333292] epoch, classifer loss: 3.970, attn_loss: 27.97437 \n",
      "[   91] iter, [0.352898] epoch, classifer loss: 3.788, attn_loss: 27.63536 \n",
      "[   96] iter, [0.372503] epoch, classifer loss: 3.647, attn_loss: 27.29928 \n",
      "[  101] iter, [0.392109] epoch, classifer loss: 3.731, attn_loss: 26.96718 \n",
      "[  106] iter, [0.411714] epoch, classifer loss: 3.933, attn_loss: 26.63965 \n",
      "[  111] iter, [0.431320] epoch, classifer loss: 4.025, attn_loss: 26.31521 \n",
      "[  116] iter, [0.450925] epoch, classifer loss: 3.903, attn_loss: 25.99337 \n",
      "[  121] iter, [0.470531] epoch, classifer loss: 3.976, attn_loss: 25.67429 \n",
      "[  126] iter, [0.490136] epoch, classifer loss: 3.846, attn_loss: 25.35857 \n",
      "[  131] iter, [0.509741] epoch, classifer loss: 3.734, attn_loss: 25.04671 \n",
      "[  136] iter, [0.529347] epoch, classifer loss: 3.903, attn_loss: 24.73704 \n",
      "[  141] iter, [0.548952] epoch, classifer loss: 3.939, attn_loss: 24.42970 \n",
      "[  146] iter, [0.568558] epoch, classifer loss: 4.015, attn_loss: 24.12592 \n",
      "[  151] iter, [0.588163] epoch, classifer loss: 3.906, attn_loss: 23.82525 \n",
      "[  156] iter, [0.607769] epoch, classifer loss: 4.015, attn_loss: 23.52919 \n",
      "[  161] iter, [0.627374] epoch, classifer loss: 3.945, attn_loss: 23.23765 \n",
      "[  166] iter, [0.646980] epoch, classifer loss: 3.953, attn_loss: 22.94947 \n",
      "[  171] iter, [0.666585] epoch, classifer loss: 3.972, attn_loss: 22.66305 \n",
      "[  176] iter, [0.686190] epoch, classifer loss: 3.901, attn_loss: 22.37926 \n",
      "[  181] iter, [0.705796] epoch, classifer loss: 3.798, attn_loss: 22.09864 \n",
      "[  186] iter, [0.725401] epoch, classifer loss: 3.986, attn_loss: 21.82194 \n",
      "[  191] iter, [0.745007] epoch, classifer loss: 4.081, attn_loss: 21.54786 \n",
      "[  196] iter, [0.764612] epoch, classifer loss: 3.948, attn_loss: 21.27583 \n",
      "[  201] iter, [0.784218] epoch, classifer loss: 3.918, attn_loss: 21.00647 \n",
      "[  206] iter, [0.803823] epoch, classifer loss: 3.796, attn_loss: 20.74114 \n",
      "[  211] iter, [0.823429] epoch, classifer loss: 3.926, attn_loss: 20.47943 \n",
      "[  216] iter, [0.843034] epoch, classifer loss: 3.767, attn_loss: 20.21961 \n",
      "[  221] iter, [0.862639] epoch, classifer loss: 4.104, attn_loss: 19.96233 \n",
      "[  226] iter, [0.882245] epoch, classifer loss: 3.937, attn_loss: 19.70894 \n",
      "[  231] iter, [0.901850] epoch, classifer loss: 3.876, attn_loss: 19.45868 \n",
      "[  236] iter, [0.921456] epoch, classifer loss: 3.900, attn_loss: 19.21139 \n",
      "[  241] iter, [0.941061] epoch, classifer loss: 3.971, attn_loss: 18.96695 \n",
      "[  246] iter, [0.960667] epoch, classifer loss: 3.942, attn_loss: 18.72539 \n",
      "[  251] iter, [0.980272] epoch, classifer loss: 4.118, attn_loss: 18.48602 \n",
      "[  256] iter, [0.999877] epoch, classifer loss: 4.295, attn_loss: 18.24894 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 36/256 [00:08<00:54,  4.00it/s]Process Process-15:\n",
      "Process Process-16:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7f2428312e10>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 8975) exited unexpectedly with exit code 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c19aeb1f977d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mturn_off_grad_except\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attn_weights'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresnet_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_k_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-8a305dd4c5c4>\u001b[0m in \u001b[0;36mtrain_k_epoch\u001b[0;34m(k, add_attn, score_epoch)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mrunning_attn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscore_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-06f7d426dcdf>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(net, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtrain_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, dest_type)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mprevious_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 8975) exited unexpectedly with exit code 1."
     ]
    }
   ],
   "source": [
    "turn_off_grad_except(['attn_weights','fc'])\n",
    "resnet_attn.eval()\n",
    "train_k_epoch(5,score_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.8107\n",
       "[torch.cuda.FloatTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_attn_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lambda = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, classifer loss: 0.739, attn_loss: 0.36215 \n",
      "[    6] iter, [0.019605] epoch, classifer loss: 4.000, attn_loss: 1.79801 \n",
      "[   11] iter, [0.039211] epoch, classifer loss: 4.009, attn_loss: 1.77731 \n",
      "[   16] iter, [0.058816] epoch, classifer loss: 4.231, attn_loss: 1.75720 \n",
      "[   21] iter, [0.078422] epoch, classifer loss: 4.062, attn_loss: 1.73770 \n",
      "[   26] iter, [0.098027] epoch, classifer loss: 4.408, attn_loss: 1.71858 \n",
      "[   31] iter, [0.117633] epoch, classifer loss: 4.088, attn_loss: 1.69986 \n",
      "[   36] iter, [0.137238] epoch, classifer loss: 4.138, attn_loss: 1.68110 \n",
      "[   41] iter, [0.156844] epoch, classifer loss: 4.120, attn_loss: 1.66234 \n",
      "[   46] iter, [0.176449] epoch, classifer loss: 3.861, attn_loss: 1.64360 \n",
      "[   51] iter, [0.196054] epoch, classifer loss: 3.969, attn_loss: 1.62500 \n",
      "[   56] iter, [0.215660] epoch, classifer loss: 3.999, attn_loss: 1.60674 \n",
      "[   61] iter, [0.235265] epoch, classifer loss: 3.966, attn_loss: 1.58895 \n",
      "[   66] iter, [0.254871] epoch, classifer loss: 3.921, attn_loss: 1.57152 \n",
      "[   71] iter, [0.274476] epoch, classifer loss: 3.939, attn_loss: 1.55458 \n",
      "[   76] iter, [0.294082] epoch, classifer loss: 3.808, attn_loss: 1.53794 \n",
      "[   81] iter, [0.313687] epoch, classifer loss: 3.762, attn_loss: 1.52148 \n",
      "[   86] iter, [0.333292] epoch, classifer loss: 3.804, attn_loss: 1.50519 \n",
      "[   91] iter, [0.352898] epoch, classifer loss: 3.790, attn_loss: 1.48914 \n",
      "[   96] iter, [0.372503] epoch, classifer loss: 3.935, attn_loss: 1.47343 \n",
      "[  101] iter, [0.392109] epoch, classifer loss: 3.797, attn_loss: 1.45799 \n",
      "[  106] iter, [0.411714] epoch, classifer loss: 3.880, attn_loss: 1.44290 \n",
      "[  111] iter, [0.431320] epoch, classifer loss: 3.972, attn_loss: 1.42822 \n",
      "[  116] iter, [0.450925] epoch, classifer loss: 4.110, attn_loss: 1.41393 \n",
      "[  121] iter, [0.470531] epoch, classifer loss: 4.032, attn_loss: 1.40001 \n",
      "[  126] iter, [0.490136] epoch, classifer loss: 3.945, attn_loss: 1.38635 \n",
      "[  131] iter, [0.509741] epoch, classifer loss: 3.901, attn_loss: 1.37276 \n",
      "[  136] iter, [0.529347] epoch, classifer loss: 3.907, attn_loss: 1.35925 \n",
      "[  141] iter, [0.548952] epoch, classifer loss: 4.008, attn_loss: 1.34586 \n",
      "[  146] iter, [0.568558] epoch, classifer loss: 3.979, attn_loss: 1.33283 \n",
      "[  151] iter, [0.588163] epoch, classifer loss: 4.106, attn_loss: 1.32043 \n",
      "[  156] iter, [0.607769] epoch, classifer loss: 3.973, attn_loss: 1.30849 \n",
      "[  161] iter, [0.627374] epoch, classifer loss: 3.801, attn_loss: 1.29680 \n",
      "[  166] iter, [0.646980] epoch, classifer loss: 3.706, attn_loss: 1.28527 \n",
      "[  171] iter, [0.666585] epoch, classifer loss: 3.891, attn_loss: 1.27373 \n",
      "[  176] iter, [0.686190] epoch, classifer loss: 3.852, attn_loss: 1.26232 \n",
      "[  181] iter, [0.705796] epoch, classifer loss: 3.828, attn_loss: 1.25121 \n",
      "[  186] iter, [0.725401] epoch, classifer loss: 3.908, attn_loss: 1.24054 \n",
      "[  191] iter, [0.745007] epoch, classifer loss: 3.943, attn_loss: 1.23010 \n",
      "[  196] iter, [0.764612] epoch, classifer loss: 3.769, attn_loss: 1.21999 \n",
      "[  201] iter, [0.784218] epoch, classifer loss: 3.882, attn_loss: 1.20994 \n",
      "[  206] iter, [0.803823] epoch, classifer loss: 3.874, attn_loss: 1.20012 \n",
      "[  211] iter, [0.823429] epoch, classifer loss: 3.880, attn_loss: 1.19055 \n",
      "[  216] iter, [0.843034] epoch, classifer loss: 4.010, attn_loss: 1.18104 \n",
      "[  221] iter, [0.862639] epoch, classifer loss: 3.923, attn_loss: 1.17194 \n",
      "[  226] iter, [0.882245] epoch, classifer loss: 4.024, attn_loss: 1.16308 \n",
      "[  231] iter, [0.901850] epoch, classifer loss: 3.977, attn_loss: 1.15442 \n",
      "[  236] iter, [0.921456] epoch, classifer loss: 3.939, attn_loss: 1.14580 \n",
      "[  241] iter, [0.941061] epoch, classifer loss: 3.762, attn_loss: 1.13714 \n",
      "[  246] iter, [0.960667] epoch, classifer loss: 3.849, attn_loss: 1.12865 \n",
      "[  251] iter, [0.980272] epoch, classifer loss: 3.821, attn_loss: 1.12035 \n",
      "[  256] iter, [0.999877] epoch, classifer loss: 4.205, attn_loss: 1.11242 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [01:03<00:00,  4.05it/s]\n",
      "100%|██████████| 55/55 [00:13<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_accu': 0.12804803332924886, 'val_accu': 0.11573546180159636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, classifer loss: 0.766, attn_loss: 0.22158 \n",
      "[    6] iter, [0.019605] epoch, classifer loss: 3.891, attn_loss: 1.10353 \n",
      "[   11] iter, [0.039211] epoch, classifer loss: 4.020, attn_loss: 1.09631 \n",
      "[   16] iter, [0.058816] epoch, classifer loss: 3.856, attn_loss: 1.08919 \n",
      "[   21] iter, [0.078422] epoch, classifer loss: 3.748, attn_loss: 1.08223 \n",
      "[   26] iter, [0.098027] epoch, classifer loss: 3.714, attn_loss: 1.07529 \n",
      "[   31] iter, [0.117633] epoch, classifer loss: 3.795, attn_loss: 1.06859 \n",
      "[   36] iter, [0.137238] epoch, classifer loss: 3.809, attn_loss: 1.06220 \n",
      "[   41] iter, [0.156844] epoch, classifer loss: 4.083, attn_loss: 1.05602 \n",
      "[   46] iter, [0.176449] epoch, classifer loss: 4.025, attn_loss: 1.05003 \n",
      "[   51] iter, [0.196054] epoch, classifer loss: 3.865, attn_loss: 1.04402 \n",
      "[   56] iter, [0.215660] epoch, classifer loss: 3.657, attn_loss: 1.03789 \n",
      "[   61] iter, [0.235265] epoch, classifer loss: 3.623, attn_loss: 1.03174 \n",
      "[   66] iter, [0.254871] epoch, classifer loss: 3.838, attn_loss: 1.02576 \n",
      "[   71] iter, [0.274476] epoch, classifer loss: 3.704, attn_loss: 1.02005 \n",
      "[   76] iter, [0.294082] epoch, classifer loss: 3.953, attn_loss: 1.01444 \n",
      "[   81] iter, [0.313687] epoch, classifer loss: 3.717, attn_loss: 1.00905 \n",
      "[   86] iter, [0.333292] epoch, classifer loss: 4.004, attn_loss: 1.00390 \n",
      "[   91] iter, [0.352898] epoch, classifer loss: 3.728, attn_loss: 0.99894 \n",
      "[   96] iter, [0.372503] epoch, classifer loss: 3.998, attn_loss: 0.99390 \n",
      "[  101] iter, [0.392109] epoch, classifer loss: 3.895, attn_loss: 0.98896 \n",
      "[  106] iter, [0.411714] epoch, classifer loss: 3.976, attn_loss: 0.98423 \n",
      "[  111] iter, [0.431320] epoch, classifer loss: 3.780, attn_loss: 0.97977 \n"
     ]
    }
   ],
   "source": [
    "turn_off_grad_except(['attn_weights','fc'])\n",
    "resnet_attn.eval()\n",
    "train_k_epoch(5,score_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
