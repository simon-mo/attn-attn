{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from AttentionModule import Conv2d_Attn\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_pretrained = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.Conv2d = Conv2d_Attn\n",
    "resnet_attn = models.resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn.load_state_dict(resnet_pretrained.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change batchnorm behavior\n",
    "# resnet_attn = resnet_attn.eval() \n",
    "# Don't want to do that because bn needs to be re-trained as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block turns 'layer1.0.downsample.0.weight' to 'layer1[0].downsample[0].weight'\n",
    "param_keys = list(resnet_attn.state_dict().keys())\n",
    "formatted_keys = []\n",
    "for k in param_keys:\n",
    "    found = re.findall(r'\\.[\\d]{1,2}\\.', k)\n",
    "    if len(found):\n",
    "        for f in found:\n",
    "            k = k.replace(f, '[{}].'.format(f.strip('.')))\n",
    "    formatted_keys.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block turn off gradient up for all params except attn_weights\n",
    "def turn_off_grad_except(lst=[]):\n",
    "    for k in formatted_keys:\n",
    "        obj = eval('resnet_attn.'+k)\n",
    "        for kw in lst:\n",
    "            if not kw in k:\n",
    "                obj.requires_grad = False\n",
    "            else:\n",
    "                obj.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn.fc = nn.Linear(resnet_attn.fc.in_features, 144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     normalize])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root='./data/train', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_imgs = len(trainset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn = resnet_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of attention parameters 26560\n"
     ]
    }
   ],
   "source": [
    "total_attn_params = 0\n",
    "for k in formatted_keys:\n",
    "    obj = eval('resnet_attn.'+k)\n",
    "    if 'attn_weights' in k:\n",
    "        total_attn_params += np.prod(obj.shape)\n",
    "print(\"Total number of attention parameters\", total_attn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the attention parameters to diverge from 1, therefore we penalize element-wise square loss as $\\lambda (1 \\times \\text{# params} - (x - 1)^2)$\n",
    "\n",
    "But this is too big a number,\n",
    "let's try: \n",
    "$- (x - 1)^2$ for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lambda = 1e-2 #set default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_objs(name, net='resnet_attn'):\n",
    "    res = []\n",
    "    for k in formatted_keys:\n",
    "        obj = eval(f'{net}.'+k)\n",
    "        if name in k:\n",
    "            res.append(obj)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attn_loss(n_params=26560):\n",
    "    attns = get_params_objs('attn_weights')\n",
    "    penality = sum([torch.pow(t - 1,2).mean() for t in attns])\n",
    "    return _lambda*(- penality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(add_attn=True):\n",
    "    cls_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet_attn.parameters()))\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_attn_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet_attn(inputs)\n",
    "        loss = cls_criterion(outputs, labels)\n",
    "        attn_loss = compute_attn_loss()\n",
    "        if add_attn:\n",
    "            loss += attn_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        running_attn_loss += attn_loss.data[0]\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print('[%5d] iter, [%2f] epoch, avg loss: %.3f, attn_loss: %.5f ' %\n",
    "                  (i + 1, i*batch_size/total_imgs, running_loss/print_every, running_attn_loss/print_every))\n",
    "            running_loss = 0.0\n",
    "            running_attn_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def score(net=resnet_attn, batch_size=batch_size):\n",
    "    trainset = torchvision.datasets.ImageFolder(root='./data/train', transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    \n",
    "    valset = torchvision.datasets.ImageFolder(root='./data/val', transform=transform)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    \n",
    "    train_correct = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    for inp, label in tqdm(iter(trainloader)):\n",
    "        _, idx = net(Variable(inp).cuda()).max(1)\n",
    "        train_correct += int(sum(idx.cpu().data == label))\n",
    "    \n",
    "    for inp, label in tqdm(iter(valloader)):\n",
    "        _, idx = net(Variable(inp).cuda()).max(1)\n",
    "        val_correct += int(sum(idx.cpu().data == label))\n",
    "    \n",
    "    return {\n",
    "        'train_accu': train_correct/len(trainset),\n",
    "        'val_accu': val_correct/len(valset)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a fresh fc layer. \n",
    "`turn_off_grad_except([])` turns off grads for all weights but the fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, avg loss: 0.994, attn_loss: 0.00000 \n",
      "[    6] iter, [0.019605] epoch, avg loss: 4.465, attn_loss: 0.00000 \n",
      "[   11] iter, [0.039211] epoch, avg loss: 4.843, attn_loss: 0.00000 \n",
      "[   16] iter, [0.058816] epoch, avg loss: 4.624, attn_loss: 0.00000 \n",
      "[   21] iter, [0.078422] epoch, avg loss: 4.628, attn_loss: 0.00000 \n",
      "[   26] iter, [0.098027] epoch, avg loss: 4.095, attn_loss: 0.00000 \n",
      "[   31] iter, [0.117633] epoch, avg loss: 4.121, attn_loss: 0.00000 \n",
      "[   36] iter, [0.137238] epoch, avg loss: 4.096, attn_loss: 0.00000 \n",
      "[   41] iter, [0.156844] epoch, avg loss: 3.978, attn_loss: 0.00000 \n",
      "[   46] iter, [0.176449] epoch, avg loss: 3.810, attn_loss: 0.00000 \n",
      "[   51] iter, [0.196054] epoch, avg loss: 3.752, attn_loss: 0.00000 \n",
      "[   56] iter, [0.215660] epoch, avg loss: 3.464, attn_loss: 0.00000 \n",
      "[   61] iter, [0.235265] epoch, avg loss: 3.433, attn_loss: 0.00000 \n",
      "[   66] iter, [0.254871] epoch, avg loss: 3.384, attn_loss: 0.00000 \n",
      "[   71] iter, [0.274476] epoch, avg loss: 3.732, attn_loss: 0.00000 \n",
      "[   76] iter, [0.294082] epoch, avg loss: 3.501, attn_loss: 0.00000 \n",
      "[   81] iter, [0.313687] epoch, avg loss: 3.312, attn_loss: 0.00000 \n",
      "[   86] iter, [0.333292] epoch, avg loss: 3.439, attn_loss: 0.00000 \n",
      "[   91] iter, [0.352898] epoch, avg loss: 3.247, attn_loss: 0.00000 \n",
      "[   96] iter, [0.372503] epoch, avg loss: 3.373, attn_loss: 0.00000 \n",
      "[  101] iter, [0.392109] epoch, avg loss: 3.485, attn_loss: 0.00000 \n",
      "[  106] iter, [0.411714] epoch, avg loss: 3.420, attn_loss: 0.00000 \n",
      "[  111] iter, [0.431320] epoch, avg loss: 3.420, attn_loss: 0.00000 \n",
      "[  116] iter, [0.450925] epoch, avg loss: 3.580, attn_loss: 0.00000 \n",
      "[  121] iter, [0.470531] epoch, avg loss: 3.130, attn_loss: 0.00000 \n",
      "[  126] iter, [0.490136] epoch, avg loss: 3.154, attn_loss: 0.00000 \n",
      "[  131] iter, [0.509741] epoch, avg loss: 3.202, attn_loss: 0.00000 \n",
      "[  136] iter, [0.529347] epoch, avg loss: 2.980, attn_loss: 0.00000 \n",
      "[  141] iter, [0.548952] epoch, avg loss: 3.343, attn_loss: 0.00000 \n",
      "[  146] iter, [0.568558] epoch, avg loss: 3.045, attn_loss: 0.00000 \n",
      "[  151] iter, [0.588163] epoch, avg loss: 2.945, attn_loss: 0.00000 \n",
      "[  156] iter, [0.607769] epoch, avg loss: 3.181, attn_loss: 0.00000 \n",
      "[  161] iter, [0.627374] epoch, avg loss: 3.104, attn_loss: 0.00000 \n",
      "[  166] iter, [0.646980] epoch, avg loss: 3.337, attn_loss: 0.00000 \n",
      "[  171] iter, [0.666585] epoch, avg loss: 2.987, attn_loss: 0.00000 \n",
      "[  176] iter, [0.686190] epoch, avg loss: 2.964, attn_loss: 0.00000 \n",
      "[  181] iter, [0.705796] epoch, avg loss: 3.302, attn_loss: 0.00000 \n",
      "[  186] iter, [0.725401] epoch, avg loss: 3.033, attn_loss: 0.00000 \n",
      "[  191] iter, [0.745007] epoch, avg loss: 2.912, attn_loss: 0.00000 \n",
      "[  196] iter, [0.764612] epoch, avg loss: 3.013, attn_loss: 0.00000 \n",
      "[  201] iter, [0.784218] epoch, avg loss: 3.017, attn_loss: 0.00000 \n",
      "[  206] iter, [0.803823] epoch, avg loss: 3.228, attn_loss: 0.00000 \n",
      "[  211] iter, [0.823429] epoch, avg loss: 3.037, attn_loss: 0.00000 \n",
      "[  216] iter, [0.843034] epoch, avg loss: 2.779, attn_loss: 0.00000 \n",
      "[  221] iter, [0.862639] epoch, avg loss: 3.051, attn_loss: 0.00000 \n",
      "[  226] iter, [0.882245] epoch, avg loss: 3.090, attn_loss: 0.00000 \n",
      "[  231] iter, [0.901850] epoch, avg loss: 2.991, attn_loss: 0.00000 \n",
      "[  236] iter, [0.921456] epoch, avg loss: 2.844, attn_loss: 0.00000 \n",
      "[  241] iter, [0.941061] epoch, avg loss: 2.814, attn_loss: 0.00000 \n",
      "[  246] iter, [0.960667] epoch, avg loss: 3.006, attn_loss: 0.00000 \n",
      "[  251] iter, [0.980272] epoch, avg loss: 3.006, attn_loss: 0.00000 \n",
      "[  256] iter, [0.999877] epoch, avg loss: 2.728, attn_loss: 0.00000 \n"
     ]
    }
   ],
   "source": [
    "turn_off_grad_except(['fc'])\n",
    "resnet_attn.eval() # Turn on batchnorm\n",
    "train_one_epoch(add_attn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [01:05<00:00,  3.89it/s]\n",
      "100%|██████████| 55/55 [00:14<00:00,  3.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_accu': 0.3810807499080995, 'val_accu': 0.3289623717217788}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, avg loss: 0.585, attn_loss: 0.00000 \n",
      "[    6] iter, [0.019605] epoch, avg loss: 3.082, attn_loss: 0.00000 \n",
      "[   11] iter, [0.039211] epoch, avg loss: 2.720, attn_loss: 0.00000 \n",
      "[   16] iter, [0.058816] epoch, avg loss: 2.738, attn_loss: 0.00000 \n",
      "[   21] iter, [0.078422] epoch, avg loss: 2.806, attn_loss: 0.00000 \n",
      "[   26] iter, [0.098027] epoch, avg loss: 2.845, attn_loss: 0.00000 \n",
      "[   31] iter, [0.117633] epoch, avg loss: 2.806, attn_loss: 0.00000 \n",
      "[   36] iter, [0.137238] epoch, avg loss: 2.669, attn_loss: 0.00000 \n",
      "[   41] iter, [0.156844] epoch, avg loss: 2.718, attn_loss: 0.00000 \n",
      "[   46] iter, [0.176449] epoch, avg loss: 2.506, attn_loss: 0.00000 \n",
      "[   51] iter, [0.196054] epoch, avg loss: 2.372, attn_loss: 0.00000 \n",
      "[   56] iter, [0.215660] epoch, avg loss: 2.670, attn_loss: 0.00000 \n",
      "[   61] iter, [0.235265] epoch, avg loss: 2.160, attn_loss: 0.00000 \n",
      "[   66] iter, [0.254871] epoch, avg loss: 2.474, attn_loss: 0.00000 \n",
      "[   71] iter, [0.274476] epoch, avg loss: 2.529, attn_loss: 0.00000 \n",
      "[   76] iter, [0.294082] epoch, avg loss: 2.425, attn_loss: 0.00000 \n",
      "[   81] iter, [0.313687] epoch, avg loss: 2.352, attn_loss: 0.00000 \n",
      "[   86] iter, [0.333292] epoch, avg loss: 2.383, attn_loss: 0.00000 \n",
      "[   91] iter, [0.352898] epoch, avg loss: 2.594, attn_loss: 0.00000 \n",
      "[   96] iter, [0.372503] epoch, avg loss: 2.420, attn_loss: 0.00000 \n",
      "[  101] iter, [0.392109] epoch, avg loss: 2.420, attn_loss: 0.00000 \n",
      "[  106] iter, [0.411714] epoch, avg loss: 2.377, attn_loss: 0.00000 \n",
      "[  111] iter, [0.431320] epoch, avg loss: 2.608, attn_loss: 0.00000 \n",
      "[  116] iter, [0.450925] epoch, avg loss: 2.372, attn_loss: 0.00000 \n",
      "[  121] iter, [0.470531] epoch, avg loss: 2.442, attn_loss: 0.00000 \n",
      "[  126] iter, [0.490136] epoch, avg loss: 2.330, attn_loss: 0.00000 \n",
      "[  131] iter, [0.509741] epoch, avg loss: 2.341, attn_loss: 0.00000 \n",
      "[  136] iter, [0.529347] epoch, avg loss: 2.308, attn_loss: 0.00000 \n",
      "[  141] iter, [0.548952] epoch, avg loss: 2.393, attn_loss: 0.00000 \n",
      "[  146] iter, [0.568558] epoch, avg loss: 2.375, attn_loss: 0.00000 \n",
      "[  151] iter, [0.588163] epoch, avg loss: 2.146, attn_loss: 0.00000 \n",
      "[  156] iter, [0.607769] epoch, avg loss: 2.093, attn_loss: 0.00000 \n",
      "[  161] iter, [0.627374] epoch, avg loss: 2.620, attn_loss: 0.00000 \n",
      "[  166] iter, [0.646980] epoch, avg loss: 2.551, attn_loss: 0.00000 \n",
      "[  171] iter, [0.666585] epoch, avg loss: 2.382, attn_loss: 0.00000 \n",
      "[  176] iter, [0.686190] epoch, avg loss: 2.400, attn_loss: 0.00000 \n",
      "[  181] iter, [0.705796] epoch, avg loss: 2.295, attn_loss: 0.00000 \n",
      "[  186] iter, [0.725401] epoch, avg loss: 2.432, attn_loss: 0.00000 \n",
      "[  191] iter, [0.745007] epoch, avg loss: 2.175, attn_loss: 0.00000 \n",
      "[  196] iter, [0.764612] epoch, avg loss: 2.239, attn_loss: 0.00000 \n",
      "[  201] iter, [0.784218] epoch, avg loss: 2.484, attn_loss: 0.00000 \n",
      "[  206] iter, [0.803823] epoch, avg loss: 2.623, attn_loss: 0.00000 \n",
      "[  211] iter, [0.823429] epoch, avg loss: 2.408, attn_loss: 0.00000 \n",
      "[  216] iter, [0.843034] epoch, avg loss: 2.352, attn_loss: 0.00000 \n",
      "[  221] iter, [0.862639] epoch, avg loss: 2.513, attn_loss: 0.00000 \n",
      "[  226] iter, [0.882245] epoch, avg loss: 2.293, attn_loss: 0.00000 \n",
      "[  231] iter, [0.901850] epoch, avg loss: 2.272, attn_loss: 0.00000 \n",
      "[  236] iter, [0.921456] epoch, avg loss: 2.505, attn_loss: 0.00000 \n",
      "[  241] iter, [0.941061] epoch, avg loss: 2.119, attn_loss: 0.00000 \n",
      "[  246] iter, [0.960667] epoch, avg loss: 2.500, attn_loss: 0.00000 \n",
      "[  251] iter, [0.980272] epoch, avg loss: 2.444, attn_loss: 0.00000 \n",
      "[  256] iter, [0.999877] epoch, avg loss: 2.986, attn_loss: 0.00000 \n"
     ]
    }
   ],
   "source": [
    "turn_off_grad_except(['attn_weights', 'bn'])\n",
    "resnet_attn.train() # Turn on batchnorm\n",
    "_lambda = 5e-1\n",
    "train_one_epoch(add_attn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [01:08<00:00,  3.73it/s]\n",
      "100%|██████████| 55/55 [00:14<00:00,  3.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_accu': 0.47077564024016666, 'val_accu': 0.3939566704675028}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, avg loss: 0.457, attn_loss: 0.00000 \n",
      "[    6] iter, [0.019605] epoch, avg loss: 1.962, attn_loss: -0.00022 \n",
      "[   11] iter, [0.039211] epoch, avg loss: 2.083, attn_loss: -0.00068 \n",
      "[   16] iter, [0.058816] epoch, avg loss: 2.131, attn_loss: -0.00115 \n",
      "[   21] iter, [0.078422] epoch, avg loss: 2.219, attn_loss: -0.00161 \n",
      "[   26] iter, [0.098027] epoch, avg loss: 1.985, attn_loss: -0.00205 \n",
      "[   31] iter, [0.117633] epoch, avg loss: 1.801, attn_loss: -0.00254 \n",
      "[   36] iter, [0.137238] epoch, avg loss: 1.849, attn_loss: -0.00318 \n",
      "[   41] iter, [0.156844] epoch, avg loss: 2.070, attn_loss: -0.00393 \n",
      "[   46] iter, [0.176449] epoch, avg loss: 1.991, attn_loss: -0.00475 \n",
      "[   51] iter, [0.196054] epoch, avg loss: 1.932, attn_loss: -0.00555 \n",
      "[   56] iter, [0.215660] epoch, avg loss: 2.007, attn_loss: -0.00638 \n",
      "[   61] iter, [0.235265] epoch, avg loss: 1.926, attn_loss: -0.00734 \n",
      "[   66] iter, [0.254871] epoch, avg loss: 1.923, attn_loss: -0.00845 \n",
      "[   71] iter, [0.274476] epoch, avg loss: 2.174, attn_loss: -0.00974 \n",
      "[   76] iter, [0.294082] epoch, avg loss: 1.747, attn_loss: -0.01119 \n",
      "[   81] iter, [0.313687] epoch, avg loss: 1.746, attn_loss: -0.01276 \n",
      "[   86] iter, [0.333292] epoch, avg loss: 1.797, attn_loss: -0.01451 \n",
      "[   91] iter, [0.352898] epoch, avg loss: 1.980, attn_loss: -0.01643 \n",
      "[   96] iter, [0.372503] epoch, avg loss: 1.870, attn_loss: -0.01848 \n",
      "[  101] iter, [0.392109] epoch, avg loss: 1.901, attn_loss: -0.02065 \n",
      "[  106] iter, [0.411714] epoch, avg loss: 1.756, attn_loss: -0.02292 \n",
      "[  111] iter, [0.431320] epoch, avg loss: 2.162, attn_loss: -0.02542 \n",
      "[  116] iter, [0.450925] epoch, avg loss: 1.772, attn_loss: -0.02802 \n",
      "[  121] iter, [0.470531] epoch, avg loss: 1.700, attn_loss: -0.03079 \n",
      "[  126] iter, [0.490136] epoch, avg loss: 1.934, attn_loss: -0.03388 \n",
      "[  131] iter, [0.509741] epoch, avg loss: 1.753, attn_loss: -0.03718 \n",
      "[  136] iter, [0.529347] epoch, avg loss: 1.851, attn_loss: -0.04068 \n",
      "[  141] iter, [0.548952] epoch, avg loss: 1.932, attn_loss: -0.04442 \n",
      "[  146] iter, [0.568558] epoch, avg loss: 1.949, attn_loss: -0.04840 \n",
      "[  151] iter, [0.588163] epoch, avg loss: 1.779, attn_loss: -0.05260 \n",
      "[  156] iter, [0.607769] epoch, avg loss: 1.508, attn_loss: -0.05706 \n",
      "[  161] iter, [0.627374] epoch, avg loss: 1.784, attn_loss: -0.06177 \n",
      "[  166] iter, [0.646980] epoch, avg loss: 1.803, attn_loss: -0.06676 \n",
      "[  171] iter, [0.666585] epoch, avg loss: 2.005, attn_loss: -0.07192 \n",
      "[  176] iter, [0.686190] epoch, avg loss: 2.178, attn_loss: -0.07728 \n",
      "[  181] iter, [0.705796] epoch, avg loss: 1.836, attn_loss: -0.08287 \n",
      "[  186] iter, [0.725401] epoch, avg loss: 1.802, attn_loss: -0.08877 \n",
      "[  191] iter, [0.745007] epoch, avg loss: 1.763, attn_loss: -0.09494 \n",
      "[  196] iter, [0.764612] epoch, avg loss: 1.758, attn_loss: -0.10147 \n",
      "[  201] iter, [0.784218] epoch, avg loss: 1.953, attn_loss: -0.10836 \n",
      "[  206] iter, [0.803823] epoch, avg loss: 1.752, attn_loss: -0.11547 \n",
      "[  211] iter, [0.823429] epoch, avg loss: 1.715, attn_loss: -0.12287 \n",
      "[  216] iter, [0.843034] epoch, avg loss: 1.710, attn_loss: -0.13059 \n",
      "[  221] iter, [0.862639] epoch, avg loss: 1.863, attn_loss: -0.13863 \n",
      "[  226] iter, [0.882245] epoch, avg loss: 1.801, attn_loss: -0.14693 \n",
      "[  231] iter, [0.901850] epoch, avg loss: 1.896, attn_loss: -0.15556 \n",
      "[  236] iter, [0.921456] epoch, avg loss: 1.653, attn_loss: -0.16453 \n",
      "[  241] iter, [0.941061] epoch, avg loss: 1.665, attn_loss: -0.17385 \n",
      "[  246] iter, [0.960667] epoch, avg loss: 1.846, attn_loss: -0.18345 \n",
      "[  251] iter, [0.980272] epoch, avg loss: 1.753, attn_loss: -0.19330 \n",
      "[  256] iter, [0.999877] epoch, avg loss: 1.303, attn_loss: -0.20355 \n"
     ]
    }
   ],
   "source": [
    "turn_off_grad_except(['attn_weights'])\n",
    "resnet_attn.eval() # Turn off batchnorm\n",
    "_lambda = 1\n",
    "train_one_epoch(add_attn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [01:06<00:00,  3.86it/s]\n",
      "100%|██████████| 55/55 [00:14<00:00,  3.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_accu': 0.5246906016419557, 'val_accu': 0.4201824401368301}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
