{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from AttentionModule import Conv2d_Attn\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_pretrained = models.resnet50(pretrained=True)\n",
    "nn.Conv2d = Conv2d_Attn\n",
    "resnet_attn = models.resnet50()\n",
    "resnet_attn.load_state_dict(resnet_pretrained.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block turns 'layer1.0.downsample.0.weight' to 'layer1[0].downsample[0].weight'\n",
    "param_keys = list(resnet_attn.state_dict().keys())\n",
    "formatted_keys = []\n",
    "for k in param_keys:\n",
    "    found = re.findall(r'\\.[\\d]{1,2}\\.', k)\n",
    "    if len(found):\n",
    "        for f in found:\n",
    "            k = k.replace(f, '[{}].'.format(f.strip('.')))\n",
    "    formatted_keys.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block turn off gradient up for all params except attn_weights\n",
    "def turn_off_grad_except(lst=[]):\n",
    "    for k in formatted_keys:\n",
    "        obj = eval('resnet_attn.'+k)\n",
    "        for kw in lst:\n",
    "            if not kw in k:\n",
    "                obj.requires_grad = False\n",
    "            else:\n",
    "                obj.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn.fc = nn.Linear(resnet_attn.fc.in_features, 144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     normalize])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root='./data/train', transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_imgs = len(trainset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn = resnet_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of attention parameters 26560\n"
     ]
    }
   ],
   "source": [
    "total_attn_params = 0\n",
    "for k in formatted_keys:\n",
    "    obj = eval('resnet_attn.'+k)\n",
    "    if 'attn_weights' in k:\n",
    "        total_attn_params += np.prod(obj.shape)\n",
    "print(\"Total number of attention parameters\", total_attn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the attention parameters to diverge from 1, therefore we penalize element-wise square loss as $\\lambda (1 \\times \\text{# params} - (x - 1)^2)$\n",
    "\n",
    "But this is too big a number,\n",
    "let's try: \n",
    "$- (x - 1)^2$ for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lambda = 1e-2 #set default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_objs(name, net='resnet_attn'):\n",
    "    res = []\n",
    "    for k in formatted_keys:\n",
    "        obj = eval(f'{net}.'+k)\n",
    "        if name in k:\n",
    "            res.append(obj)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attn_loss(n_params=26560):\n",
    "    attns = get_params_objs('attn_weights')\n",
    "    penality = sum([torch.pow(t - 1,2).mean() for t in attns])\n",
    "    return _lambda*(- penality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(add_attn=True):\n",
    "    cls_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet_attn.parameters()))\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_attn_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet_attn(inputs)\n",
    "        loss = cls_criterion(outputs, labels)\n",
    "        attn_loss = compute_attn_loss()\n",
    "        if add_attn:\n",
    "            loss += attn_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        running_loss += loss.data[0]\n",
    "        running_attn_loss += attn_loss.data[0]\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print('[%5d] iter, [%2f] epoch, avg loss: %.3f, attn_loss: %.5f ' %\n",
    "                  (i + 1, i*batch_size/total_imgs, running_loss/print_every, running_attn_loss/print_every))\n",
    "            running_loss = 0.0\n",
    "            running_attn_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def score(net=resnet_attn, batch_size=batch_size):\n",
    "    trainset = torchvision.datasets.ImageFolder(root='./data/train', transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    \n",
    "    valset = torchvision.datasets.ImageFolder(root='./data/val', transform=transform)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    \n",
    "    train_correct = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    for inp, label in tqdm(iter(trainloader)):\n",
    "        _, idx = net(Variable(inp).cuda()).topk(3)\n",
    "        train_correct += int(sum(idx.cpu().data == label))\n",
    "    \n",
    "    for inp, label in tqdm(iter(valloader)):\n",
    "        _, idx = net(Variable(inp).cuda()).topk(3)\n",
    "        val_correct += int(sum(idx.cpu().data == label))\n",
    "    \n",
    "    return {\n",
    "        'train_accu': train_correct/len(trainset),\n",
    "        'val_accu': val_correct/len(valset)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a fresh fc layer. \n",
    "`turn_off_grad_except([])` turns off grads for all weights but the fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, avg loss: 1.016, attn_loss: 0.00000 \n",
      "[    6] iter, [0.019605] epoch, avg loss: 4.964, attn_loss: 0.00000 \n",
      "[   11] iter, [0.039211] epoch, avg loss: 4.760, attn_loss: 0.00000 \n",
      "[   16] iter, [0.058816] epoch, avg loss: 4.470, attn_loss: 0.00000 \n",
      "[   21] iter, [0.078422] epoch, avg loss: 4.109, attn_loss: 0.00000 \n",
      "[   26] iter, [0.098027] epoch, avg loss: 4.484, attn_loss: 0.00000 \n",
      "[   31] iter, [0.117633] epoch, avg loss: 4.379, attn_loss: 0.00000 \n",
      "[   36] iter, [0.137238] epoch, avg loss: 3.903, attn_loss: 0.00000 \n",
      "[   41] iter, [0.156844] epoch, avg loss: 4.099, attn_loss: 0.00000 \n",
      "[   46] iter, [0.176449] epoch, avg loss: 3.918, attn_loss: 0.00000 \n",
      "[   51] iter, [0.196054] epoch, avg loss: 4.117, attn_loss: 0.00000 \n",
      "[   56] iter, [0.215660] epoch, avg loss: 3.676, attn_loss: 0.00000 \n",
      "[   61] iter, [0.235265] epoch, avg loss: 3.515, attn_loss: 0.00000 \n",
      "[   66] iter, [0.254871] epoch, avg loss: 3.373, attn_loss: 0.00000 \n",
      "[   71] iter, [0.274476] epoch, avg loss: 3.595, attn_loss: 0.00000 \n",
      "[   76] iter, [0.294082] epoch, avg loss: 3.368, attn_loss: 0.00000 \n",
      "[   81] iter, [0.313687] epoch, avg loss: 3.468, attn_loss: 0.00000 \n",
      "[   86] iter, [0.333292] epoch, avg loss: 3.874, attn_loss: 0.00000 \n",
      "[   91] iter, [0.352898] epoch, avg loss: 3.314, attn_loss: 0.00000 \n",
      "[   96] iter, [0.372503] epoch, avg loss: 3.225, attn_loss: 0.00000 \n",
      "[  101] iter, [0.392109] epoch, avg loss: 3.296, attn_loss: 0.00000 \n",
      "[  106] iter, [0.411714] epoch, avg loss: 3.138, attn_loss: 0.00000 \n",
      "[  111] iter, [0.431320] epoch, avg loss: 3.178, attn_loss: 0.00000 \n",
      "[  116] iter, [0.450925] epoch, avg loss: 3.490, attn_loss: 0.00000 \n",
      "[  121] iter, [0.470531] epoch, avg loss: 3.191, attn_loss: 0.00000 \n",
      "[  126] iter, [0.490136] epoch, avg loss: 2.991, attn_loss: 0.00000 \n",
      "[  131] iter, [0.509741] epoch, avg loss: 3.151, attn_loss: 0.00000 \n",
      "[  136] iter, [0.529347] epoch, avg loss: 3.139, attn_loss: 0.00000 \n",
      "[  141] iter, [0.548952] epoch, avg loss: 3.019, attn_loss: 0.00000 \n",
      "[  146] iter, [0.568558] epoch, avg loss: 2.965, attn_loss: 0.00000 \n",
      "[  151] iter, [0.588163] epoch, avg loss: 3.158, attn_loss: 0.00000 \n",
      "[  156] iter, [0.607769] epoch, avg loss: 3.108, attn_loss: 0.00000 \n",
      "[  161] iter, [0.627374] epoch, avg loss: 3.123, attn_loss: 0.00000 \n",
      "[  166] iter, [0.646980] epoch, avg loss: 3.042, attn_loss: 0.00000 \n",
      "[  171] iter, [0.666585] epoch, avg loss: 3.035, attn_loss: 0.00000 \n",
      "[  176] iter, [0.686190] epoch, avg loss: 3.207, attn_loss: 0.00000 \n",
      "[  181] iter, [0.705796] epoch, avg loss: 3.414, attn_loss: 0.00000 \n",
      "[  186] iter, [0.725401] epoch, avg loss: 3.171, attn_loss: 0.00000 \n",
      "[  191] iter, [0.745007] epoch, avg loss: 2.974, attn_loss: 0.00000 \n",
      "[  196] iter, [0.764612] epoch, avg loss: 3.053, attn_loss: 0.00000 \n",
      "[  201] iter, [0.784218] epoch, avg loss: 3.055, attn_loss: 0.00000 \n",
      "[  206] iter, [0.803823] epoch, avg loss: 3.088, attn_loss: 0.00000 \n",
      "[  211] iter, [0.823429] epoch, avg loss: 3.005, attn_loss: 0.00000 \n",
      "[  216] iter, [0.843034] epoch, avg loss: 2.922, attn_loss: 0.00000 \n",
      "[  221] iter, [0.862639] epoch, avg loss: 3.298, attn_loss: 0.00000 \n",
      "[  226] iter, [0.882245] epoch, avg loss: 2.988, attn_loss: 0.00000 \n",
      "[  231] iter, [0.901850] epoch, avg loss: 2.928, attn_loss: 0.00000 \n",
      "[  236] iter, [0.921456] epoch, avg loss: 2.812, attn_loss: 0.00000 \n",
      "[  241] iter, [0.941061] epoch, avg loss: 2.867, attn_loss: 0.00000 \n",
      "[  246] iter, [0.960667] epoch, avg loss: 2.902, attn_loss: 0.00000 \n",
      "[  251] iter, [0.980272] epoch, avg loss: 2.884, attn_loss: 0.00000 \n",
      "[  256] iter, [0.999877] epoch, avg loss: 2.566, attn_loss: 0.00000 \n"
     ]
    }
   ],
   "source": [
    "turn_off_grad_except(['fc'])\n",
    "resnet_attn.eval() # Turn on batchnorm\n",
    "train_one_epoch(add_attn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset = torchvision.datasets.ImageFolder(root='./data/val', transform=transform)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = 0\n",
    "for inp, label in iter(valloader):\n",
    "    _, idx = resnet_attn(Variable(inp).cuda()).topk(3)\n",
    "    lab = Variable(label).cuda()\n",
    "    lab_expand = lab.unsqueeze(1).expand_as(idx)\n",
    "    correct_count += int((idx == lab_expand).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5245153933865451"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count/len(valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, avg loss: 0.486, attn_loss: 0.00000 \n",
      "[    6] iter, [0.019605] epoch, avg loss: 2.573, attn_loss: -0.00020 \n",
      "[   11] iter, [0.039211] epoch, avg loss: 2.478, attn_loss: -0.00065 \n",
      "[   16] iter, [0.058816] epoch, avg loss: 2.402, attn_loss: -0.00120 \n",
      "[   21] iter, [0.078422] epoch, avg loss: 2.339, attn_loss: -0.00191 \n",
      "[   26] iter, [0.098027] epoch, avg loss: 2.246, attn_loss: -0.00275 \n",
      "[   31] iter, [0.117633] epoch, avg loss: 2.399, attn_loss: -0.00365 \n",
      "[   36] iter, [0.137238] epoch, avg loss: 2.384, attn_loss: -0.00455 \n",
      "[   41] iter, [0.156844] epoch, avg loss: 2.384, attn_loss: -0.00555 \n",
      "[   46] iter, [0.176449] epoch, avg loss: 2.359, attn_loss: -0.00664 \n",
      "[   51] iter, [0.196054] epoch, avg loss: 2.202, attn_loss: -0.00788 \n",
      "[   56] iter, [0.215660] epoch, avg loss: 2.175, attn_loss: -0.00929 \n",
      "[   61] iter, [0.235265] epoch, avg loss: 2.203, attn_loss: -0.01086 \n",
      "[   66] iter, [0.254871] epoch, avg loss: 2.054, attn_loss: -0.01259 \n",
      "[   71] iter, [0.274476] epoch, avg loss: 2.118, attn_loss: -0.01445 \n",
      "[   76] iter, [0.294082] epoch, avg loss: 2.471, attn_loss: -0.01645 \n",
      "[   81] iter, [0.313687] epoch, avg loss: 2.425, attn_loss: -0.01860 \n",
      "[   86] iter, [0.333292] epoch, avg loss: 2.420, attn_loss: -0.02099 \n",
      "[   91] iter, [0.352898] epoch, avg loss: 2.066, attn_loss: -0.02358 \n",
      "[   96] iter, [0.372503] epoch, avg loss: 2.204, attn_loss: -0.02646 \n",
      "[  101] iter, [0.392109] epoch, avg loss: 1.979, attn_loss: -0.02968 \n",
      "[  106] iter, [0.411714] epoch, avg loss: 2.040, attn_loss: -0.03315 \n",
      "[  111] iter, [0.431320] epoch, avg loss: 2.173, attn_loss: -0.03671 \n",
      "[  116] iter, [0.450925] epoch, avg loss: 2.137, attn_loss: -0.04046 \n",
      "[  121] iter, [0.470531] epoch, avg loss: 2.159, attn_loss: -0.04455 \n",
      "[  126] iter, [0.490136] epoch, avg loss: 2.051, attn_loss: -0.04879 \n",
      "[  131] iter, [0.509741] epoch, avg loss: 2.100, attn_loss: -0.05328 \n",
      "[  136] iter, [0.529347] epoch, avg loss: 2.392, attn_loss: -0.05809 \n",
      "[  141] iter, [0.548952] epoch, avg loss: 2.276, attn_loss: -0.06307 \n",
      "[  146] iter, [0.568558] epoch, avg loss: 2.290, attn_loss: -0.06832 \n",
      "[  151] iter, [0.588163] epoch, avg loss: 2.241, attn_loss: -0.07389 \n",
      "[  156] iter, [0.607769] epoch, avg loss: 1.959, attn_loss: -0.07972 \n",
      "[  161] iter, [0.627374] epoch, avg loss: 1.996, attn_loss: -0.08596 \n",
      "[  166] iter, [0.646980] epoch, avg loss: 2.001, attn_loss: -0.09254 \n",
      "[  171] iter, [0.666585] epoch, avg loss: 2.184, attn_loss: -0.09940 \n",
      "[  176] iter, [0.686190] epoch, avg loss: 2.102, attn_loss: -0.10643 \n",
      "[  181] iter, [0.705796] epoch, avg loss: 2.166, attn_loss: -0.11382 \n",
      "[  186] iter, [0.725401] epoch, avg loss: 2.149, attn_loss: -0.12163 \n",
      "[  191] iter, [0.745007] epoch, avg loss: 2.477, attn_loss: -0.12976 \n",
      "[  196] iter, [0.764612] epoch, avg loss: 2.102, attn_loss: -0.13804 \n",
      "[  201] iter, [0.784218] epoch, avg loss: 2.241, attn_loss: -0.14661 \n",
      "[  206] iter, [0.803823] epoch, avg loss: 1.882, attn_loss: -0.15562 \n",
      "[  211] iter, [0.823429] epoch, avg loss: 1.891, attn_loss: -0.16505 \n",
      "[  216] iter, [0.843034] epoch, avg loss: 2.251, attn_loss: -0.17483 \n",
      "[  221] iter, [0.862639] epoch, avg loss: 1.740, attn_loss: -0.18485 \n",
      "[  226] iter, [0.882245] epoch, avg loss: 1.889, attn_loss: -0.19525 \n",
      "[  231] iter, [0.901850] epoch, avg loss: 2.070, attn_loss: -0.20606 \n",
      "[  236] iter, [0.921456] epoch, avg loss: 1.995, attn_loss: -0.21707 \n",
      "[  241] iter, [0.941061] epoch, avg loss: 1.804, attn_loss: -0.22838 \n",
      "[  246] iter, [0.960667] epoch, avg loss: 1.856, attn_loss: -0.24007 \n",
      "[  251] iter, [0.980272] epoch, avg loss: 2.095, attn_loss: -0.25217 \n",
      "[  256] iter, [0.999877] epoch, avg loss: 1.396, attn_loss: -0.26473 \n"
     ]
    }
   ],
   "source": [
    "turn_off_grad_except(['attn_weights'])\n",
    "resnet_attn.eval() # Turn on batchnorm\n",
    "_lambda = 1\n",
    "train_one_epoch(add_attn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:14<00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5872291904218928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "for inp, label in tqdm(iter(valloader)):\n",
    "    _, idx = resnet_attn(Variable(inp).cuda()).topk(3)\n",
    "    lab = Variable(label).cuda()\n",
    "    lab_expand = lab.unsqueeze(1).expand_as(idx)\n",
    "    correct_count += int((idx == lab_expand).sum())\n",
    "print(correct_count/len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_top3():\n",
    "    correct_count = 0\n",
    "    for inp, label in tqdm(iter(valloader)):\n",
    "        _, idx = resnet_attn(Variable(inp).cuda()).topk(3)\n",
    "        lab = Variable(label).cuda()\n",
    "        lab_expand = lab.unsqueeze(1).expand_as(idx)\n",
    "        correct_count += int((idx == lab_expand).sum())\n",
    "    print(correct_count/len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, avg loss: 0.418, attn_loss: -0.05450 \n",
      "[    6] iter, [0.019605] epoch, avg loss: 1.597, attn_loss: -0.27796 \n",
      "[   11] iter, [0.039211] epoch, avg loss: 1.961, attn_loss: -0.28708 \n",
      "[   16] iter, [0.058816] epoch, avg loss: 1.766, attn_loss: -0.29619 \n",
      "[   21] iter, [0.078422] epoch, avg loss: 2.021, attn_loss: -0.30543 \n",
      "[   26] iter, [0.098027] epoch, avg loss: 1.909, attn_loss: -0.31477 \n",
      "[   31] iter, [0.117633] epoch, avg loss: 1.771, attn_loss: -0.32432 \n",
      "[   36] iter, [0.137238] epoch, avg loss: 1.895, attn_loss: -0.33424 \n",
      "[   41] iter, [0.156844] epoch, avg loss: 1.767, attn_loss: -0.34440 \n",
      "[   46] iter, [0.176449] epoch, avg loss: 1.812, attn_loss: -0.35471 \n",
      "[   51] iter, [0.196054] epoch, avg loss: 1.832, attn_loss: -0.36517 \n",
      "[   56] iter, [0.215660] epoch, avg loss: 1.652, attn_loss: -0.37587 \n",
      "[   61] iter, [0.235265] epoch, avg loss: 1.763, attn_loss: -0.38702 \n",
      "[   66] iter, [0.254871] epoch, avg loss: 1.826, attn_loss: -0.39857 \n",
      "[   71] iter, [0.274476] epoch, avg loss: 1.810, attn_loss: -0.41035 \n",
      "[   76] iter, [0.294082] epoch, avg loss: 1.455, attn_loss: -0.42228 \n",
      "[   81] iter, [0.313687] epoch, avg loss: 1.637, attn_loss: -0.43450 \n",
      "[   86] iter, [0.333292] epoch, avg loss: 1.515, attn_loss: -0.44702 \n",
      "[   91] iter, [0.352898] epoch, avg loss: 1.585, attn_loss: -0.45992 \n",
      "[   96] iter, [0.372503] epoch, avg loss: 1.502, attn_loss: -0.47328 \n",
      "[  101] iter, [0.392109] epoch, avg loss: 1.661, attn_loss: -0.48696 \n",
      "[  106] iter, [0.411714] epoch, avg loss: 1.593, attn_loss: -0.50091 \n",
      "[  111] iter, [0.431320] epoch, avg loss: 1.464, attn_loss: -0.51513 \n",
      "[  116] iter, [0.450925] epoch, avg loss: 1.599, attn_loss: -0.52971 \n",
      "[  121] iter, [0.470531] epoch, avg loss: 1.459, attn_loss: -0.54486 \n",
      "[  126] iter, [0.490136] epoch, avg loss: 1.572, attn_loss: -0.56033 \n",
      "[  131] iter, [0.509741] epoch, avg loss: 1.458, attn_loss: -0.57615 \n",
      "[  136] iter, [0.529347] epoch, avg loss: 1.411, attn_loss: -0.59233 \n",
      "[  141] iter, [0.548952] epoch, avg loss: 1.459, attn_loss: -0.60894 \n",
      "[  146] iter, [0.568558] epoch, avg loss: 1.543, attn_loss: -0.62586 \n",
      "[  151] iter, [0.588163] epoch, avg loss: 1.571, attn_loss: -0.64300 \n",
      "[  156] iter, [0.607769] epoch, avg loss: 1.532, attn_loss: -0.66045 \n",
      "[  161] iter, [0.627374] epoch, avg loss: 1.319, attn_loss: -0.67857 \n",
      "[  166] iter, [0.646980] epoch, avg loss: 1.523, attn_loss: -0.69707 \n",
      "[  171] iter, [0.666585] epoch, avg loss: 1.144, attn_loss: -0.71589 \n",
      "[  176] iter, [0.686190] epoch, avg loss: 1.268, attn_loss: -0.73510 \n",
      "[  181] iter, [0.705796] epoch, avg loss: 1.161, attn_loss: -0.75482 \n",
      "[  186] iter, [0.725401] epoch, avg loss: 1.161, attn_loss: -0.77501 \n",
      "[  191] iter, [0.745007] epoch, avg loss: 1.024, attn_loss: -0.79553 \n",
      "[  196] iter, [0.764612] epoch, avg loss: 1.048, attn_loss: -0.81635 \n",
      "[  201] iter, [0.784218] epoch, avg loss: 0.963, attn_loss: -0.83776 \n",
      "[  206] iter, [0.803823] epoch, avg loss: 1.089, attn_loss: -0.85992 \n",
      "[  211] iter, [0.823429] epoch, avg loss: 1.022, attn_loss: -0.88249 \n",
      "[  216] iter, [0.843034] epoch, avg loss: 1.145, attn_loss: -0.90557 \n",
      "[  221] iter, [0.862639] epoch, avg loss: 1.404, attn_loss: -0.92920 \n",
      "[  226] iter, [0.882245] epoch, avg loss: 1.169, attn_loss: -0.95311 \n",
      "[  231] iter, [0.901850] epoch, avg loss: 0.878, attn_loss: -0.97752 \n",
      "[  236] iter, [0.921456] epoch, avg loss: 1.201, attn_loss: -1.00273 \n",
      "[  241] iter, [0.941061] epoch, avg loss: 1.312, attn_loss: -1.02830 \n",
      "[  246] iter, [0.960667] epoch, avg loss: 0.820, attn_loss: -1.05416 \n",
      "[  251] iter, [0.980272] epoch, avg loss: 0.923, attn_loss: -1.08070 \n",
      "[  256] iter, [0.999877] epoch, avg loss: 1.003, attn_loss: -1.10792 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:14<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6088939566704675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "turn_off_grad_except(['attn_weights'])\n",
    "resnet_attn.eval() # Turn on batchnorm\n",
    "_lambda = 1\n",
    "train_one_epoch(add_attn=True)\n",
    "score_top3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "attns = get_params_objs('attn_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "attns = torch.cat([attn.squeeze() for attn in attns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "attns_arr = attns.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  260.,    84.,   164.,   971., 12044., 11546.,   989.,   167.,\n",
       "           82.,   253.]),\n",
       " array([0.31773689, 0.45415982, 0.59058275, 0.72700568, 0.86342861,\n",
       "        0.99985154, 1.13627447, 1.2726974 , 1.40912033, 1.54554326,\n",
       "        1.68196619]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(attns_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, avg loss: 0.201, attn_loss: -0.22491 \n",
      "[    6] iter, [0.019605] epoch, avg loss: 0.914, attn_loss: -1.13828 \n",
      "[   11] iter, [0.039211] epoch, avg loss: 0.851, attn_loss: -1.16156 \n",
      "[   16] iter, [0.058816] epoch, avg loss: 1.016, attn_loss: -1.18541 \n",
      "[   21] iter, [0.078422] epoch, avg loss: 0.810, attn_loss: -1.20941 \n",
      "[   26] iter, [0.098027] epoch, avg loss: 0.570, attn_loss: -1.23363 \n",
      "[   31] iter, [0.117633] epoch, avg loss: 0.805, attn_loss: -1.25819 \n",
      "[   36] iter, [0.137238] epoch, avg loss: 0.780, attn_loss: -1.28330 \n",
      "[   41] iter, [0.156844] epoch, avg loss: 0.720, attn_loss: -1.30885 \n",
      "[   46] iter, [0.176449] epoch, avg loss: 0.417, attn_loss: -1.33479 \n",
      "[   51] iter, [0.196054] epoch, avg loss: 0.404, attn_loss: -1.36131 \n",
      "[   56] iter, [0.215660] epoch, avg loss: 0.739, attn_loss: -1.38832 \n",
      "[   61] iter, [0.235265] epoch, avg loss: 0.849, attn_loss: -1.41593 \n",
      "[   66] iter, [0.254871] epoch, avg loss: 0.417, attn_loss: -1.44391 \n",
      "[   71] iter, [0.274476] epoch, avg loss: 0.577, attn_loss: -1.47228 \n",
      "[   76] iter, [0.294082] epoch, avg loss: 0.345, attn_loss: -1.50115 \n",
      "[   81] iter, [0.313687] epoch, avg loss: 0.461, attn_loss: -1.53069 \n",
      "[   86] iter, [0.333292] epoch, avg loss: 0.656, attn_loss: -1.56084 \n",
      "[   91] iter, [0.352898] epoch, avg loss: 0.471, attn_loss: -1.59135 \n",
      "[   96] iter, [0.372503] epoch, avg loss: 0.172, attn_loss: -1.62234 \n",
      "[  101] iter, [0.392109] epoch, avg loss: 0.119, attn_loss: -1.65412 \n",
      "[  106] iter, [0.411714] epoch, avg loss: 0.045, attn_loss: -1.68644 \n",
      "[  111] iter, [0.431320] epoch, avg loss: 0.046, attn_loss: -1.71912 \n",
      "[  116] iter, [0.450925] epoch, avg loss: 0.341, attn_loss: -1.75213 \n",
      "[  121] iter, [0.470531] epoch, avg loss: 0.077, attn_loss: -1.78555 \n",
      "[  126] iter, [0.490136] epoch, avg loss: 0.052, attn_loss: -1.81962 \n",
      "[  131] iter, [0.509741] epoch, avg loss: -0.027, attn_loss: -1.85400 \n",
      "[  136] iter, [0.529347] epoch, avg loss: -0.205, attn_loss: -1.88914 \n",
      "[  141] iter, [0.548952] epoch, avg loss: 0.043, attn_loss: -1.92503 \n",
      "[  146] iter, [0.568558] epoch, avg loss: 0.319, attn_loss: -1.96135 \n",
      "[  151] iter, [0.588163] epoch, avg loss: -0.004, attn_loss: -1.99806 \n",
      "[  156] iter, [0.607769] epoch, avg loss: -0.133, attn_loss: -2.03559 \n",
      "[  161] iter, [0.627374] epoch, avg loss: -0.191, attn_loss: -2.07378 \n",
      "[  166] iter, [0.646980] epoch, avg loss: -0.142, attn_loss: -2.11256 \n",
      "[  171] iter, [0.666585] epoch, avg loss: -0.150, attn_loss: -2.15205 \n",
      "[  176] iter, [0.686190] epoch, avg loss: -0.129, attn_loss: -2.19239 \n",
      "[  181] iter, [0.705796] epoch, avg loss: -0.228, attn_loss: -2.23356 \n",
      "[  186] iter, [0.725401] epoch, avg loss: -0.533, attn_loss: -2.27527 \n",
      "[  191] iter, [0.745007] epoch, avg loss: -0.383, attn_loss: -2.31769 \n",
      "[  196] iter, [0.764612] epoch, avg loss: -0.413, attn_loss: -2.36077 \n",
      "[  201] iter, [0.784218] epoch, avg loss: -0.472, attn_loss: -2.40438 \n",
      "[  206] iter, [0.803823] epoch, avg loss: -0.282, attn_loss: -2.44847 \n",
      "[  211] iter, [0.823429] epoch, avg loss: -0.377, attn_loss: -2.49342 \n",
      "[  216] iter, [0.843034] epoch, avg loss: -0.855, attn_loss: -2.53915 \n",
      "[  221] iter, [0.862639] epoch, avg loss: -0.816, attn_loss: -2.58572 \n",
      "[  226] iter, [0.882245] epoch, avg loss: -0.719, attn_loss: -2.63296 \n",
      "[  231] iter, [0.901850] epoch, avg loss: -0.862, attn_loss: -2.68091 \n",
      "[  236] iter, [0.921456] epoch, avg loss: -0.472, attn_loss: -2.72965 \n",
      "[  241] iter, [0.941061] epoch, avg loss: -0.980, attn_loss: -2.77902 \n",
      "[  246] iter, [0.960667] epoch, avg loss: -1.032, attn_loss: -2.82919 \n",
      "[  251] iter, [0.980272] epoch, avg loss: -1.105, attn_loss: -2.88012 \n",
      "[  256] iter, [0.999877] epoch, avg loss: -1.206, attn_loss: -2.93161 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:14<00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61345496009122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "turn_off_grad_except(['attn_weights'])\n",
    "resnet_attn.eval() # Turn on batchnorm\n",
    "_lambda = 1\n",
    "train_one_epoch(add_attn=True)\n",
    "score_top3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
