{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from AttentionModule import Conv2d_Attn\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_pretrained = models.resnet50(pretrained=True)\n",
    "nn.Conv2d = Conv2d_Attn\n",
    "resnet_attn = models.resnet50()\n",
    "resnet_attn.load_state_dict(resnet_pretrained.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block turns 'layer1.0.downsample.0.weight' to 'layer1[0].downsample[0].weight'\n",
    "param_keys = list(resnet_attn.state_dict().keys())\n",
    "formatted_keys = []\n",
    "for k in param_keys:\n",
    "    found = re.findall(r'\\.[\\d]{1,2}\\.', k)\n",
    "    if len(found):\n",
    "        for f in found:\n",
    "            k = k.replace(f, '[{}].'.format(f.strip('.')))\n",
    "    formatted_keys.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block turn off gradient up for all params except attn_weights\n",
    "def turn_off_grad_except(lst=[]):\n",
    "    for k in formatted_keys:\n",
    "        obj = eval('resnet_attn.'+k)\n",
    "        for kw in lst:\n",
    "            if not kw in k:\n",
    "                obj.requires_grad = False\n",
    "            else:\n",
    "                obj.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn.fc = nn.Linear(resnet_attn.fc.in_features, 144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# count number of instances for each class and use sampler for class imbalance\n",
    "TRAIN_DIR = '/home/bdrad1/ryan/194/data/train'\n",
    "VAL_DIR = '/home/bdrad1/ryan/194/data/val'\n",
    "TEST_DIR = '/home/bdrad1/ryan/194/data/test'\n",
    "\n",
    "classes = os.listdir(TRAIN_DIR)\n",
    "classes.remove('.DS_Store')\n",
    "class_counts = [len(os.listdir(os.path.join(TRAIN_DIR, c))) for c in classes]\n",
    "\n",
    "c = 0\n",
    "weights = []\n",
    "for directory, _, files in os.walk(TRAIN_DIR):\n",
    "    if not directory.endswith('train'):\n",
    "        for f in files:\n",
    "            weights.append(class_counts[c])       \n",
    "\n",
    "weights = [1.0/i for i in weights]\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, 8161, replacement= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=TRAIN_DIR, transform=transform)\n",
    "valset = torchvision.datasets.ImageFolder(root=VAL_DIR, transform = transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2, sampler = sampler)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training images 8161\n"
     ]
    }
   ],
   "source": [
    "total_imgs = len(trainset.imgs)\n",
    "print('number of training images', total_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn = resnet_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of attention parameters 13385920\n"
     ]
    }
   ],
   "source": [
    "total_attn_params = 0\n",
    "for k in formatted_keys:\n",
    "    obj = eval('resnet_attn.'+k)\n",
    "    if 'attn_weights' in k:\n",
    "        total_attn_params += np.prod(obj.shape)\n",
    "print(\"Total number of attention parameters\", total_attn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the attention parameters to diverge from 1, therefore we penalize element-wise square loss as $\\lambda (1 \\times \\text{# params} - (x - 1)^2)$\n",
    "\n",
    "But this is too big a number,\n",
    "let's try: \n",
    "$- (x - 1)^2$ for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_lambda = 1e-2 #set default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_params_objs(name, net='resnet_attn'):\n",
    "    res = []\n",
    "    for k in formatted_keys:\n",
    "        obj = eval(f'{net}.'+k)\n",
    "        if name in k:\n",
    "            res.append(obj)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_attn_loss(n_params=26560):\n",
    "    attns = get_params_objs('attn_weights')\n",
    "    penality = sum([torch.pow(t - 1,2).mean() for t in attns])\n",
    "    return _lambda*(- penality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(add_attn=True, lr = 0.01):\n",
    "    cls_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet_attn.parameters()), lr=lr)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_attn_loss = 0.0\n",
    "    training_corrects1 = 0\n",
    "    training_corrects3 = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet_attn(inputs)\n",
    "        \n",
    "\n",
    "        loss = cls_criterion(outputs, labels)\n",
    "        attn_loss = compute_attn_loss()\n",
    "        if add_attn:\n",
    "            loss += attn_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #calculate training acc\n",
    "        _, idx1 = outputs.topk(1)\n",
    "        _, idx3 = outputs.topk(3)\n",
    "        lab_expand1 = labels.unsqueeze(1).expand_as(idx1)\n",
    "        lab_expand3 = labels.unsqueeze(1).expand_as(idx3)\n",
    "        training_corrects1 += int((idx1 == lab_expand1).sum())\n",
    "        training_corrects3 += int((idx3 == lab_expand3).sum())\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        running_attn_loss += attn_loss.data[0]\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print('[%5d] iter, [%2f] epoch, avg loss: %.3f, attn_loss: %.5f ' %\n",
    "                  (i + 1, i*batch_size/total_imgs, running_loss/print_every, running_attn_loss/print_every))\n",
    "            running_loss = 0.0\n",
    "            running_attn_loss = 0.0\n",
    "    training_acc1 = training_corrects1 / total_imgs\n",
    "    training_acc3 = training_corrects3 / total_imgs\n",
    "    return training_acc1, training_acc3, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def score(net=resnet_attn, batch_size=batch_size):\n",
    "    trainset = torchvision.datasets.ImageFolder(root=TRAIN_DIR, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    \n",
    "    valset = torchvision.datasets.ImageFolder(root=VAL_DIR, transform=transform)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    \n",
    "    train_correct = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    for inp, label in tqdm(iter(trainloader)):\n",
    "        _, idx = net(Variable(inp).cuda()).topk(3)\n",
    "        train_correct += int(sum(idx.cpu().data == label))\n",
    "    \n",
    "    for inp, label in tqdm(iter(valloader)):\n",
    "        _, idx = net(Variable(inp).cuda()).topk(3)\n",
    "        val_correct += int(sum(idx.cpu().data == label))\n",
    "    \n",
    "    return {\n",
    "        'train_accu': train_correct/len(trainset),\n",
    "        'val_accu': val_correct/len(valset)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a fresh fc layer. \n",
    "`turn_off_grad_except([])` turns off grads for all weights but the fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_top3():\n",
    "    correct_count = 0\n",
    "    for inp, label in tqdm(iter(valloader)):\n",
    "        _, idx = resnet_attn(Variable(inp).cuda()).topk(3)\n",
    "        lab = Variable(label).cuda()\n",
    "        lab_expand = lab.unsqueeze(1).expand_as(idx)\n",
    "        correct_count += int((idx == lab_expand).sum())\n",
    "    print(correct_count/len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training scheme\n",
    "print_every = 50\n",
    "val_accs = []\n",
    "def train(seq = ['fc', 'att', 'fc', 'bn', 'att', 'att', 'bn', 'att', 'fc','att']):\n",
    "    start = time.time()\n",
    "    dir_name = ''\n",
    "    for x in seq:\n",
    "        dir_name += x[0]\n",
    "    if not os.path.isdir('checkpoints/'+dir_name):\n",
    "        os.mkdir('checkpoints/'+dir_name)\n",
    "    sys.stdout = open('logs/'+dir_name+'.txt', 'w')\n",
    "    \n",
    "    \n",
    "    highest_acc1 = 0\n",
    "    for idx, s in enumerate(seq):\n",
    "        print('======================= epoch:', idx,'layer:',s,\"=========================\")\n",
    "        if s == 'fc':\n",
    "            turn_off_grad_except(['fc'])\n",
    "        elif s == 'att':\n",
    "            turn_off_grad_except(['attn_weights'])\n",
    "        elif s == 'bn':\n",
    "            turn_off_grad_except(['bn'])\n",
    "        training_acc1, training_acc3 ,optimizer = train_one_epoch()\n",
    "        \n",
    "        correct_count1, correct_count3 = 0,0\n",
    "        \n",
    "        for inp, label in iter(valloader):\n",
    "            _, idx1 = resnet_attn(Variable(inp).cuda()).topk(1)\n",
    "            _, idx3 = resnet_attn(Variable(inp).cuda()).topk(3)\n",
    "            \n",
    "            lab = Variable(label).cuda()\n",
    "            lab_expand1 = lab.unsqueeze(1).expand_as(idx1)\n",
    "            lab_expand3 = lab.unsqueeze(1).expand_as(idx3)\n",
    "            correct_count1 += int((idx1 == lab_expand1).sum())\n",
    "            correct_count3 += int((idx3 == lab_expand3).sum())\n",
    "            \n",
    "        val_acc_1 = correct_count1/len(valset)\n",
    "        val_acc_3 = correct_count3/len(valset)\n",
    "        time_elapsed = time.time() - start\n",
    "        \n",
    "        if val_acc_1 > highest_acc1 and idx > 2:\n",
    "            #Save best acc model state dict\n",
    "            state = {\n",
    "            'epoch': idx,\n",
    "            'time': '{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60),\n",
    "            'arch': seq,\n",
    "            'state_dict': resnet_attn.state_dict(),\n",
    "            'train_acc1': training_acc1,\n",
    "            'train_acc3': training_acc3,\n",
    "            'val_acc1': val_acc_1,\n",
    "            'val_acc3': val_acc_3,\n",
    "            'optimizer' : optimizer.state_dict()\n",
    "            }\n",
    "            save_checkpoint(state, 'epoch{}_val1_{:.3f}_val3_{:.3f}.pth'.format(idx, val_acc_1, val_acc_3), 'checkpoints/'+dir_name)\n",
    "        print(\"top 1 train_acc:{:.4f} | top 3 train_acc:{:.4f} | top 1 val_acc: {:.4f} | top 3 val_acc: {:.4f} | time: {:.0f}m {:.0f}s\\n\".format(\n",
    "            training_acc1, training_acc3, val_acc_1, val_acc_3, time_elapsed // 60, time_elapsed % 60\n",
    "        ))\n",
    "        val_accs.append(val_acc_1)\n",
    "    return highest_acc1, val_accs\n",
    "    \n",
    "def save_checkpoint(state, filename, cp_path):\n",
    "    torch.save(state, os.path.join(cp_path, filename))\n",
    "    print(\"saved model to {}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 55, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/datasets/folder.py\", line 124, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py\", line 42, in __call__\n",
      "    img = t(img)\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py\", line 61, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/functional.py\", line 76, in to_tensor\n",
      "    return img.float().div(255)\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/site-packages/torch/tensor.py\", line 53, in float\n",
      "    return self.type(type(self).__module__ + '.FloatTensor')\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/site-packages/torch/_utils.py\", line 38, in _type\n",
      "    return new_type(self.size()).copy_(self, async)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7f2420287780>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 333, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 319, in _shutdown_workers\n",
      "    self.data_queue.get()\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 487, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/bdrad1/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 614, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e624b7532eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc_t1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-05a245d8791f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mturn_off_grad_except\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtraining_acc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_acc3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcorrect_count1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_count3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-bfd1132582ff>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(add_attn)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtraining_corrects3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc_t1, val_accs = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoints/fbafbaabaaabaaabaa/epoch13_val1_0.495_val3_0.695.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn.load_state_dict(checkpoint['state_dict'])\n",
    "resnet_attn.cuda()\n",
    "#test on best model\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=TEST_DIR, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    correct_count1, correct_count3 = 0, 0\n",
    "\n",
    "    for inp, label in iter(testloader):\n",
    "        _, idx1 = model(Variable(inp).cuda()).topk(1)\n",
    "        _, idx3 = model(Variable(inp).cuda()).topk(3)\n",
    "\n",
    "        lab = Variable(label).cuda()\n",
    "        lab_expand1 = lab.unsqueeze(1).expand_as(idx1)\n",
    "        lab_expand3 = lab.unsqueeze(1).expand_as(idx3)\n",
    "        correct_count1 += int((idx1 == lab_expand1).sum())\n",
    "        correct_count3 += int((idx3 == lab_expand3).sum())\n",
    "\n",
    "    test_acc_1 = correct_count1/len(testset)\n",
    "    test_acc_3 = correct_count3/len(testset)\n",
    "    print(\"top 1 test acc:{}, top 3 test acc:{}\".format(test_acc_1, test_acc_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test(resnet_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train different architectures\n",
    "architectures = [\n",
    "    ['fc', 'att', 'fc', 'bn', 'att', 'att', 'bn', 'att', 'fc','att'],\n",
    "    ['fc','fc', 'bn', 'att', 'bn','att','bn', 'att','bn','att'],\n",
    "    ['fc','fc', 'bn','att','att','bn','att','att','bn','att','att'],\n",
    "    ['fc','bn','att','fc','bn','att','fc','bn','att','att'],\n",
    "    ['fc','fc','att','att','att','att','att','att']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for arc in architectures:\n",
    "#     resnet_attn.fc = nn.Linear(resnet_attn.fc.in_features, 1000)\n",
    "#     resnet_attn.load_state_dict(resnet_pretrained.state_dict(), strict=False)\n",
    "#     resnet_attn.fc = nn.Linear(resnet_attn.fc.in_features, 144)\n",
    "#     resnet_attn.cuda()\n",
    "#     train(arc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " [0.2554161915621437,\n",
       "  0.35689851767388825,\n",
       "  0.3979475484606613,\n",
       "  0.3899657924743444,\n",
       "  0.4161915621436716,\n",
       "  0.44982896237172176,\n",
       "  0.4623717217787913,\n",
       "  0.45153933865450396,\n",
       "  0.4657924743443558,\n",
       "  0.47263397947548463,\n",
       "  0.4771949828962372,\n",
       "  0.475484606613455,\n",
       "  0.49144811858608894,\n",
       "  0.49543899657924745,\n",
       "  0.49372862029646525,\n",
       "  0.4977194982896237,\n",
       "  0.4863169897377423,\n",
       "  0.4965792474344356])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resnet_attn.fc = nn.Linear(resnet_attn.fc.in_features, 1000)\n",
    "# resnet_attn.load_state_dict(resnet_pretrained.state_dict(), strict=False)\n",
    "# resnet_attn.fc = nn.Linear(resnet_attn.fc.in_features, 144)\n",
    "# resnet_attn.cuda()\n",
    "train(['fc','bn','att','fc','bn','att','att','bn','att','att','att','bn','att','att','att','bn','att','att'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1] iter, [0.000000] epoch, avg loss: 0.013, attn_loss: -0.00008 \n",
      "[   51] iter, [0.196054] epoch, avg loss: 1.327, attn_loss: -0.00421 \n",
      "[  101] iter, [0.392109] epoch, avg loss: 1.308, attn_loss: -0.00421 \n",
      "[  151] iter, [0.588163] epoch, avg loss: 1.289, attn_loss: -0.00421 \n",
      "[  201] iter, [0.784218] epoch, avg loss: 1.189, attn_loss: -0.00422 \n",
      "[  251] iter, [0.980272] epoch, avg loss: 1.115, attn_loss: -0.00422 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:03<00:00, 17.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6459521094640821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "resnet_attn.load_state_dict(checkpoint['state_dict'])\n",
    "resnet_attn.cuda()\n",
    "train_one_epoch(add_attn=False, lr = 0.0001)\n",
    "score_top3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
