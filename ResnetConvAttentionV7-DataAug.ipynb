{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from AttentionModule import Conv2d_Attn\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_pretrained = models.resnet50(pretrained=True)\n",
    "nn.Conv2d = Conv2d_Attn\n",
    "resnet_attn = models.resnet50()\n",
    "resnet_attn.load_state_dict(resnet_pretrained.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block turns 'layer1.0.downsample.0.weight' to 'layer1[0].downsample[0].weight'\n",
    "param_keys = list(resnet_attn.state_dict().keys())\n",
    "formatted_keys = []\n",
    "for k in param_keys:\n",
    "    found = re.findall(r'\\.[\\d]{1,2}\\.', k)\n",
    "    if len(found):\n",
    "        for f in found:\n",
    "            k = k.replace(f, '[{}].'.format(f.strip('.')))\n",
    "    formatted_keys.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This block turn off gradient up for all params except attn_weights\n",
    "def turn_off_grad_except(lst=[]):\n",
    "    for k in formatted_keys:\n",
    "        obj = eval('resnet_attn.'+k)\n",
    "        for kw in lst:\n",
    "            if not kw in k:\n",
    "                obj.requires_grad = False\n",
    "            else:\n",
    "                obj.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn.fc = nn.Linear(resnet_attn.fc.in_features, 144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# count number of instances for each class and use sampler for class imbalance\n",
    "TRAIN_DIR = '/home/bdrad1/ryan/194/data/train'\n",
    "VAL_DIR = '/home/bdrad1/ryan/194/data/val'\n",
    "TEST_DIR = '/home/bdrad1/ryan/194/data/test'\n",
    "\n",
    "classes = os.listdir(TRAIN_DIR)\n",
    "classes.remove('.DS_Store')\n",
    "class_counts = [len(os.listdir(os.path.join(TRAIN_DIR, c))) for c in classes]\n",
    "\n",
    "c = 0\n",
    "weights = []\n",
    "for directory, _, files in os.walk(TRAIN_DIR):\n",
    "    if not directory.endswith('train'):\n",
    "        for f in files:\n",
    "            weights.append(class_counts[c])       \n",
    "\n",
    "weights = [1.0/i for i in weights]\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, 8161, replacement= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(root=TRAIN_DIR, transform=transform)\n",
    "valset = torchvision.datasets.ImageFolder(root=VAL_DIR, transform = transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2, sampler = sampler)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training images 8161\n"
     ]
    }
   ],
   "source": [
    "total_imgs = len(trainset.imgs)\n",
    "print('number of training images', total_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_attn = resnet_attn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of attention parameters 13385920\n"
     ]
    }
   ],
   "source": [
    "total_attn_params = 0\n",
    "for k in formatted_keys:\n",
    "    obj = eval('resnet_attn.'+k)\n",
    "    if 'attn_weights' in k:\n",
    "        total_attn_params += np.prod(obj.shape)\n",
    "print(\"Total number of attention parameters\", total_attn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the attention parameters to diverge from 1, therefore we penalize element-wise square loss as $\\lambda (1 \\times \\text{# params} - (x - 1)^2)$\n",
    "\n",
    "But this is too big a number,\n",
    "let's try: \n",
    "$- (x - 1)^2$ for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_lambda = 1e-2 #set default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_params_objs(name, net='resnet_attn'):\n",
    "    res = []\n",
    "    for k in formatted_keys:\n",
    "        obj = eval(f'{net}.'+k)\n",
    "        if name in k:\n",
    "            res.append(obj)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_attn_loss(n_params=26560):\n",
    "    attns = get_params_objs('attn_weights')\n",
    "    penality = sum([torch.pow(t - 1,2).mean() for t in attns])\n",
    "    return _lambda*(- penality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(add_attn=True):\n",
    "    cls_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, resnet_attn.parameters()))\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_attn_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet_attn(inputs)\n",
    "        \n",
    "\n",
    "        loss = cls_criterion(outputs, labels)\n",
    "        attn_loss = compute_attn_loss()\n",
    "        if add_attn:\n",
    "            loss += attn_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #calculate training acc\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels)\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        running_attn_loss += attn_loss.data[0]\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print('[%5d] iter, [%2f] epoch, avg loss: %.3f, attn_loss: %.5f ' %\n",
    "                  (i + 1, i*batch_size/total_imgs, running_loss/print_every, running_attn_loss/print_every))\n",
    "            running_loss = 0.0\n",
    "            running_attn_loss = 0.0\n",
    "    training_acc = running_corrects.double() / total_imgs\n",
    "    return training_acc, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def score(net=resnet_attn, batch_size=batch_size):\n",
    "    trainset = torchvision.datasets.ImageFolder(root=TRAIN_DIR, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    \n",
    "    valset = torchvision.datasets.ImageFolder(root=VAL_DIR, transform=transform)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    \n",
    "    train_correct = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    for inp, label in tqdm(iter(trainloader)):\n",
    "        _, idx = net(Variable(inp).cuda()).topk(3)\n",
    "        train_correct += int(sum(idx.cpu().data == label))\n",
    "    \n",
    "    for inp, label in tqdm(iter(valloader)):\n",
    "        _, idx = net(Variable(inp).cuda()).topk(3)\n",
    "        val_correct += int(sum(idx.cpu().data == label))\n",
    "    \n",
    "    return {\n",
    "        'train_accu': train_correct/len(trainset),\n",
    "        'val_accu': val_correct/len(valset)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a fresh fc layer. \n",
    "`turn_off_grad_except([])` turns off grads for all weights but the fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_top3():\n",
    "    correct_count = 0\n",
    "    for inp, label in tqdm(iter(valloader)):\n",
    "        _, idx = resnet_attn(Variable(inp).cuda()).topk(3)\n",
    "        lab = Variable(label).cuda()\n",
    "        lab_expand = lab.unsqueeze(1).expand_as(idx)\n",
    "        correct_count += int((idx == lab_expand).sum())\n",
    "    print(correct_count/len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training scheme\n",
    "print_every = 50\n",
    "val_accs = []\n",
    "def train(seq = ['fc', 'att', 'fc', 'bn', 'att', 'att', 'bn', 'att', 'fc','att']):\n",
    "    dir_name = ''\n",
    "    for x in seq:\n",
    "        dir_name += x[0]\n",
    "    if not os.direxists('checkpoints/'+dir_name):\n",
    "        os.mkdir('checkpoints/'+dir_name)\n",
    "    highest_acc1 = 0\n",
    "    for idx, s in enumerate(seq):\n",
    "        print('======================= epoch:', idx,'layer:',s,\"=========================\")\n",
    "        if s == 'fc':\n",
    "            turn_off_grad_except(['fc'])\n",
    "        elif s == 'att':\n",
    "            turn_off_grad_except(['attn_weights'])\n",
    "        elif s == 'bn':\n",
    "            turn_off_grad_except(['bn'])\n",
    "        training_acc,optimizer = train_one_epoch()\n",
    "        \n",
    "        correct_count1, correct_count3 = 0,0\n",
    "        \n",
    "        for inp, label in iter(valloader):\n",
    "            _, idx1 = resnet_attn(Variable(inp).cuda()).topk(1)\n",
    "            _, idx3 = resnet_attn(Variable(inp).cuda()).topk(3)\n",
    "            \n",
    "            lab = Variable(label).cuda()\n",
    "            lab_expand1 = lab.unsqueeze(1).expand_as(idx1)\n",
    "            lab_expand3 = lab.unsqueeze(1).expand_as(idx3)\n",
    "            correct_count1 += int((idx1 == lab_expand1).sum())\n",
    "            correct_count3 += int((idx3 == lab_expand3).sum())\n",
    "            \n",
    "        val_acc_1 = correct_count1/len(valset)\n",
    "        val_acc_3 = correct_count3/len(valset)\n",
    "        \n",
    "        if val_acc_1 > highest_acc1:\n",
    "            highest_acc1 = val_acc_1\n",
    "            #Save best acc model state dict\n",
    "            state = {\n",
    "            'epoch': idx,\n",
    "            'arch': seq,\n",
    "            'state_dict': resnet_attn.state_dict(),\n",
    "            'val_acc1': val_acc_1,\n",
    "            'val_acc3': val_acc_3,\n",
    "            'optimizer' : optimizer.state_dict()\n",
    "            }\n",
    "            save_checkpoint(state, 'epoch{}_val1_{:.3f}_val3_{:.3f}.pth'.format(idx, val_acc_1, val_acc_3), 'checkpoints/'+dir_name)\n",
    "        print(\"top 1 val_acc: {} top 3 val_acc: {}\".format(val_acc_1, val_acc_3))\n",
    "        val_accs.append(val_acc_1)\n",
    "    return highest_acc1, val_accs\n",
    "    \n",
    "def save_checkpoint(state, filename, cp_path):\n",
    "    torch.save(state, os.path.join(cp_path, filename))\n",
    "    print(\"saved model to {}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'direxists'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e624b7532eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc_t1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-60722ef98304>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdir_name\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mhighest_acc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'direxists'"
     ]
    }
   ],
   "source": [
    "acc_t1, val_accs = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoints/5-8/epoch9_val1_0.47263397947548463_val3_0.6835803876852907.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_attn.load_state_dict(checkpoint['state_dict'])\n",
    "resnet_attn.cuda()\n",
    "#test on best model\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=TEST_DIR, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    correct_count1, correct_count3 = 0, 0\n",
    "\n",
    "    for inp, label in iter(testloader):\n",
    "        _, idx1 = model(Variable(inp).cuda()).topk(1)\n",
    "        _, idx3 = model(Variable(inp).cuda()).topk(3)\n",
    "\n",
    "        lab = Variable(label).cuda()\n",
    "        lab_expand1 = lab.unsqueeze(1).expand_as(idx1)\n",
    "        lab_expand3 = lab.unsqueeze(1).expand_as(idx3)\n",
    "        correct_count1 += int((idx1 == lab_expand1).sum())\n",
    "        correct_count3 += int((idx3 == lab_expand3).sum())\n",
    "\n",
    "    test_acc_1 = correct_count1/len(testset)\n",
    "    test_acc_3 = correct_count3/len(testset)\n",
    "    print(\"top 1 test acc:{}, top 3 test acc:{}\".format(test_acc_1, test_acc_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 1 test acc:0.4734010759115362, top 3 test acc:0.6826060968320382\n"
     ]
    }
   ],
   "source": [
    "test(resnet_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dir(x):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sv'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'s'+'v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
